<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>WP-006-24Dec2023</title>
      <link href="/2023/12/24/WP-006-24Dec2023/"/>
      <url>/2023/12/24/WP-006-24Dec2023/</url>
      
        <content type="html"><![CDATA[<h2 id="一、本周做了什么"><a href="#一、本周做了什么" class="headerlink" title="一、本周做了什么"></a>一、本周做了什么</h2><ol><li><p>读AI辅助GNSS outages的文献<br> 阅读了十几篇AI辅助GNSS outages相关领域的文献，并进行了整理，将作为Related work部分</p></li><li><p>完成了LSTM付出GNSS定位算法仿真实验、流程图绘制等<br> 本周对上周初步的仿真结果进行了参数调整，详见附件PPT</p></li><li><p>花了两天时间做了NN直接预测伪距和多普勒的实验，效果不佳，暂时放弃</p></li><li><p>整理近期工作的PPT并准备组会汇报</p></li></ol><h2 id="二、下周计划"><a href="#二、下周计划" class="headerlink" title="二、下周计划"></a>二、下周计划</h2><ol><li>增加实验对比（RTKLIB、VINS-Fusion等），归纳整理</li><li>购买ublox zed f9p定位模块并学习使用方法</li><li>将定位模块安装在小车上，并去室外采集实验数据</li></ol><p>附件：本阶段工作整理PPT</p>]]></content>
      
      
      <categories>
          
          <category> NaN </category>
          
          <category> NaN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WeeklyReport-005-17Dec2023</title>
      <link href="/2023/12/17/WeeklyReport-005-17Dec2023/"/>
      <url>/2023/12/17/WeeklyReport-005-17Dec2023/</url>
      
        <content type="html"><![CDATA[<h1 id="工作周报"><a href="#工作周报" class="headerlink" title=" 工作周报 "></a><p style="text-align: center;"> 工作周报 </p></h1><p style="text-align: center;"> <b>时间: </b> 2023-12-11 ~ 2023-12-17 </p><br><h2 id="一、本周做了什么"><a href="#一、本周做了什么" class="headerlink" title="一、本周做了什么"></a>一、本周做了什么</h2><h3 id="1-增加训练集-测试集数据，并进行训练"><a href="#1-增加训练集-测试集数据，并进行训练" class="headerlink" title="1. 增加训练集&#x2F;测试集数据，并进行训练"></a>1. 增加训练集&#x2F;测试集数据，并进行训练</h3><p>上周用于实验的数据太少了，导致模型训练出来的效果不好，本周对数据集进行了扩充，序列长度由5min增加到25min.<br>目前可供使用的数据有：</p><p>原始量测：IMU（a w）、GNSS（psr、 dopplar）<br>初步处理：IMU预积分数据、<br>融合输出：GVINS结果（GNSS有效）、VINS结果（GNSS无效）<br>真值系统：RTK真值</p><p>扩充后训练效果好了很多，效果展示如下：</p><p><strong>数据集：</strong><br>sports_field —– 5mins(4mins-train 1min-eval)<br>数据集：1510秒，train:eval &#x3D; 8：2 &#x3D; 20mins:5mins</p><p><strong>NN 描述：</strong><br>2 layers LSTM with 64 hidden layer<br>loss funcation : MSE<br>optimizer:Adam betas&#x3D;(0.9, 0.999), eps&#x3D;1e-08, weight_decay&#x3D;0</p><p>input: imu pre-integration position<br>output: GVINS fusion position</p><p><strong>参数情况：</strong><br>Epoches:100<br>learn_rate:0.01</p><p><strong>结果描述</strong></p><p>Model name: lstm_N100_lr0.01_20231216-094112.pth (Available)</p><p>验证效果变好，eval_loss&#x3D;0.0005173331317691145<br>loss 曲线：<br><img src="img%20(2).png" alt="Alt text"></p><p>验证效果：<br><img src="img%20(1).png" alt="Alt text"></p><p>参数还需要精调</p><h3 id="2-完成效果评估系统搭建"><a href="#2-完成效果评估系统搭建" class="headerlink" title="2. 完成效果评估系统搭建"></a>2. 完成效果评估系统搭建</h3><p>打通从传感器量测到融合输出到评估系统的全流程。例如使用evo评估工具对融合结果进行评估：</p><p>sport-field.bag<br>截取前5分钟数据<br>仅评估位置，不评估姿态（gt无姿态）</p><p>evo对齐方式：仅原点对齐</p><h2 id="Case0-对照-no-align-GVINS-GNSS无故障原始输出"><a href="#Case0-对照-no-align-GVINS-GNSS无故障原始输出" class="headerlink" title="Case0 [对照-no align] GVINS- GNSS无故障原始输出"></a>Case0 [对照-no align] GVINS- GNSS无故障原始输出</h2><p><strong>轨迹：</strong><br><img src="image.png" alt="Alt text"><br><img src="image-2.png" alt="Alt text"><br><img src="image-1.png" alt="Alt text"><br><strong>ape结果：</strong><br><img src="image-4.png" alt="Alt text"><br><img src="image-3.png" alt="Alt text"><br><strong>rpe结果：</strong><br><img src="image-5.png" alt="Alt text"><br><img src="image-6.png" alt="Alt text"></p><h2 id="Case1-对照-纯VINS"><a href="#Case1-对照-纯VINS" class="headerlink" title="Case1 [对照] 纯VINS"></a>Case1 [对照] 纯VINS</h2><p><strong>轨迹：</strong><br><img src="image-7.png" alt="Alt text"><br><img src="image-10.png" alt="Alt text"><br><img src="image-11.png" alt="Alt text"></p><p><strong>ape结果：</strong><br><img src="image-8.png" alt="Alt text"><br><img src="image-9.png" alt="Alt text"></p><p><strong>rpe结果：</strong><br><img src="image-12.png" alt="Alt text"><br><img src="image-13.png" alt="Alt text"></p><p>在数据指标方面，选取了下图的指标（也是论文中常用的指标）进行评估：<br><img src="image.png" alt="Alt text"></p><h3 id="3-快速过了一遍深度学习的视频"><a href="#3-快速过了一遍深度学习的视频" class="headerlink" title="3. 快速过了一遍深度学习的视频"></a>3. 快速过了一遍深度学习的视频</h3><p>花了一天的时间过了一遍深度学习的基础，算是扫盲<br>主要包含：<br>NN的基本知识（NN结构、损失函数、得分函数、forward、backward、训练的步骤、optimizer(Adam etc)）<br>CNN基础（卷积操作、step、padding、空洞卷积）<br>RNN基础、LSTM基础（结构、forward过程）</p><p>并整理了关键点笔记</p><h3 id="4-学习-修改LSTM网络，并整理了自己在学习期间的一些感悟"><a href="#4-学习-修改LSTM网络，并整理了自己在学习期间的一些感悟" class="headerlink" title="4. 学习&#x2F;修改LSTM网络，并整理了自己在学习期间的一些感悟"></a>4. 学习&#x2F;修改LSTM网络，并整理了自己在学习期间的一些感悟</h3><p>深入的学习的LSTM的原理，对：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">单点预测（已知0到T-1，预测T）</span><br><span class="line">多点多长度预测（已知0到T，预测T+N ~ T+N+M）</span><br><span class="line">多长度预测（已知0到T，预测T+1 ~ T+N）</span><br></pre></td></tr></table></figure><p>进行了深入研究，并结合股票预测的问题进行了验证，期间有一些心得体会，整理如下:</p><p><strong>LSTM学习有感-股票预测</strong></p><p>最近在研究LSTM预测相关知识，搞了两天，发现自己似乎进了一个大坑，记录一下这两天干的事和一些感悟</p><p><strong>1. 实验过程描述</strong></p><p>一条股票【收盘价】数据，85% 的数据用于训练，15% 的数据用于测试<br>2层LSTM，64个隐藏层<br>LEARNING_RATE &#x3D; 1e-2, epoch &#x3D; 150<br>loss是mse, adam优化器， drop_out&#x3D;20%<br>采样滑窗50</p><p>单点预测：<br>用1-49天真实数据跑出第50天的预测数据<br>然后用2-50天的真实数据跑出第51天预测数据,<br>以此类推. 得到了一组预测数据</p><p><strong>效果：</strong><br><strong>训练loss曲线：</strong><br><img src="image-110.png" alt="Alt text"></p><p>test loss &#x3D; 0.0009153251885436475 (很小)</p><p><img src="image-111.png" alt="Alt text"><br>效果好到离谱， 趋势很对</p><p>** 2. 错误原因分析**</p><p>lstm从纯股价并没学到有效信息，但是在单点预测中，连续两天股价变化其实很小，并且loss函数式mse，所以模型默认用上一天的数据加一点拟合的噪声来当作预测值。这样loss最小，但其实没有意义。</p><p>个人觉得评价指标和损失选得不行，没能反应问题的实质。</p><h2 id="3-一些tips"><a href="#3-一些tips" class="headerlink" title="3. 一些tips"></a>3. 一些tips</h2><ol><li>金融市场做预测，要预测收益率，不要预测价格，反应到车辆轨迹预测，要预测运动趋势的变动（速度、加速度变化），不要直接预测位置</li></ol><h3 id="5-学习不同GNSS的时间框架，并整理"><a href="#5-学习不同GNSS的时间框架，并整理" class="headerlink" title="5. 学习不同GNSS的时间框架，并整理"></a>5. 学习不同GNSS的时间框架，并整理</h3><p>在扩充数据集的时候，遇到了UTC、GNSS不同星座的时间同步问题，这是一个非常恼人的问题，故而去仔细学习了这方面内容，并做了整理。已经上传到了我的个人博客，链接如下：<br><a href="https://www.damonai.cn/2023/12/14/GNSS%E6%97%B6%E9%97%B4%E7%B3%BB%E7%BB%9F%E5%8F%8A%E5%85%B6%E8%BD%AC%E6%8D%A2/">https://www.damonai.cn/2023/12/14/GNSS%E6%97%B6%E9%97%B4%E7%B3%BB%E7%BB%9F%E5%8F%8A%E5%85%B6%E8%BD%AC%E6%8D%A2/</a> </p><h2 id="二、下周怎么安排"><a href="#二、下周怎么安排" class="headerlink" title="二、下周怎么安排"></a>二、下周怎么安排</h2><ol><li>搜索相关文件，对当前朴素的网络进行修改，并对网络参数进行调整优化</li><li>对目前已有的、下周即将开展的实验结果进行整理（指标、图表等）</li><li>结合文献，对当前整个GNSS deny环境下的定位思路进行梳理</li></ol>]]></content>
      
      
      <categories>
          
          <category> NaN </category>
          
          <category> NaN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>GNSS时间系统及其转换</title>
      <link href="/2023/12/14/GNSS%E6%97%B6%E9%97%B4%E7%B3%BB%E7%BB%9F%E5%8F%8A%E5%85%B6%E8%BD%AC%E6%8D%A2/"/>
      <url>/2023/12/14/GNSS%E6%97%B6%E9%97%B4%E7%B3%BB%E7%BB%9F%E5%8F%8A%E5%85%B6%E8%BD%AC%E6%8D%A2/</url>
      
        <content type="html"><![CDATA[<h1 id="GNSS-时间系统及其转换"><a href="#GNSS-时间系统及其转换" class="headerlink" title="GNSS 时间系统及其转换"></a>GNSS 时间系统及其转换</h1><h2 id="1-时间单位-原子时秒"><a href="#1-时间单位-原子时秒" class="headerlink" title="1. 时间单位-原子时秒"></a>1. 时间单位-原子时秒</h2><p>原子时秒长的定义为：位于海平面上的铬133原于基态两个超精细能级，在零磁场中跃迁辐射振荡9192631770周所持续的时间，为1原子时秒。</p><h2 id="2-时间框架和其时间起点"><a href="#2-时间框架和其时间起点" class="headerlink" title="2. 时间框架和其时间起点"></a>2. 时间框架和其时间起点</h2><h3 id="2-1-原子时（AT）"><a href="#2-1-原子时（AT）" class="headerlink" title="2.1 原子时（AT）"></a>2.1 原子时（AT）</h3><p>原子时起点定在1958年1月1日0时0分0秒（UT），即规定在这一瞬间原子时时刻与世界时刻重合。但事后发现，在该瞬间原子时与世界时的时刻之差为0.0039秒。这一差值就作为历史事实而保留下来。即：</p><p>$ UT &#x3D; AT + 0.0039 $</p><p>原子时的出现，在全球各国获得迅速的应用，但不同地方的原子时之间存在着差异。为此，国际时间局对世界上精选出的100座原子钟进行相互比对，经数据处理推算出统一的原子时系统，称为国际原子时(international atomic time，IAT)。</p><h2 id="2-2-世界时（UT）"><a href="#2-2-世界时（UT）" class="headerlink" title="2.2 世界时（UT）"></a>2.2 世界时（UT）</h2><p>地球上零经度子午圈(格林尼治子午圈)所对应的平太阳时且以平子夜为零时起算的时间系统，称为世界时（UT）。所谓平太阳时，是指通过观测太阳连续两 次经过本地子午圈的时间间隔为一个平太阳日。</p><p>世界时是以地球自转为基础定义的。但是，如前所述，地球自转的速度并不均匀，且自转轴的方向在地球内部亦不固定(极移现象)。这样，地球自转的不稳定性，就违背了建立时间系统的基本条件。</p><p>为了弥补这—缺陷，自1956年以来，便在世界时UT中引入了极移改正项Δλ和季节性改正ΔTS，由此获得的世界时用UT1和UT2来表示，未经改正的世界时用UT0来表示</p><h3 id="2-3-协调世界时（UTC）"><a href="#2-3-协调世界时（UTC）" class="headerlink" title="2.3 协调世界时（UTC）"></a>2.3 协调世界时（UTC）</h3><p>原子时的秒长比世界时的秒长略短，这就使原子时比世界时每年约快ls(多出1s)。两者之差逐年积累。为了避免发播的原子时与世界时之间产生过大的偏差，同时，又要使两种时间系统同时并存，就有必要建立一种兼有两种时间系统各自优点的新的时间。</p><p>这就是从1972年起所采用的协调版界时(Coordinated Universal Time， UTC，简称协调时)。</p><p>协调世界时（UTC）的秒长，<strong>严格等于原子时的秒长</strong>，采用<strong>闰秒</strong>(或称跳秒)的办法使协调时与世界时的时刻相接近，当协调时与世界时的时刻差超过±0.9s时，便在协调时中引入一闰秒(或正或负)，闰秒一般在12月31日或6月30日的最后一秒加入。具体日期由国际时间局安排并通告。</p><p>协调时与国际原子时之间的关系，如下式所示：</p><p> $ IAT &#x3D; UTC + 1*n $</p><p>式中：n为调整参数，其值由国际地球自转服务组织（IERS）发布。</p><h3 id="2-4-GPST"><a href="#2-4-GPST" class="headerlink" title="2.4 GPST"></a>2.4 GPST</h3><p>GPS时间系统采用<strong>原子时</strong>系统，以美国海军天文台（USNO）维护的协调世界时（UTC）作为基准。GPS时间与原子时TAI名义上相差一个常数</p><p>在GPS标准历元1980年1月6日0时，GPS时间与UTC一致。TAI与UTC相差整n秒。2014年7月，整数值n&#x3D;35，也就是说GPS时间比UTC早16s。</p><p>GPS时间系统在表示时间时采用GPS周和周秒来标示GPS系统时间。所采用的最大时间单位为周（Week，即604800秒）， 其标示时间的方法是从1980年1月6日0时开始起算的周数（Week Number，WN）加上被称为周内时间（Time of Week，TOW）的从每周周六&#x2F;周日子夜开始起算的秒数。在GPS卫星所发送的导航电文中， 时间信息的标示就是采用这样的形式。</p><p>通常，GPS时间要快于UTC时间，快的多少由1980年1月6日0时后的闰秒次数有关，最近一次闰秒在北京时间2017年1月1日7时59分59秒，此时，GPS时间和UTC时间的关系为：</p><p>$ GPST &#x3D; UTC + 18.0 $</p><h3 id="2-5-BDT"><a href="#2-5-BDT" class="headerlink" title="2.5 BDT"></a>2.5 BDT</h3><p>北斗系统的时间基准为北斗时（BDT）。</p><p>BDT 采用国际单位制（SI）秒为基本单位连续累计，<strong>不跳秒</strong>，起始历元为2006 年1 月1 日协调世界时（UTC）00 时00 分00 秒，采用周和周内秒计数。BDT 通过UTC（NTSC）与国际UTC 建立联系，<strong>BDT 与UTC 的偏差保持在100 纳秒以内（模1秒</strong>）。<strong>BDT与UTC之间的跳秒信息在导航电文中播报。</strong></p><p>虽然GPS与BDS都采用原子时作为时间基准，且均属于连续的时间系统，但时间起算原点不同，由于UTC存在跳秒，因此<strong>GPST和BDT与UTC分别相差整数跳秒</strong>，而<strong>GPST与BDT时间起点不同，故GPST与BDT也相差一个整数跳秒</strong></p><p>$ BDT &#x3D; GPST - 14$<br>$ BDT &#x3D; UTC  +  4$ </p><h3 id="2-6-GLO时间参考基准"><a href="#2-6-GLO时间参考基准" class="headerlink" title="2.6 GLO时间参考基准"></a>2.6 GLO时间参考基准</h3><p>GLONASS时间系统是整个GLONASS系统的时间基准，它属于UTC时间系统，但是以俄罗斯维持的世界协调时UTC（SU）作为时间度量基准，且有一个3小时的时间偏移，即莫斯科时间与格林尼治时差。</p><p>$ GLOT &#x3D; UTC + 3h $</p><h3 id="2-7-GAL时间参考基准"><a href="#2-7-GAL时间参考基准" class="headerlink" title="2.7 GAL时间参考基准"></a>2.7 GAL时间参考基准</h3><p>GAL系统使用GPST作为时间参考基准。</p><h3 id="2-8-Unix或POSIX时间戳："><a href="#2-8-Unix或POSIX时间戳：" class="headerlink" title="2.8 Unix或POSIX时间戳："></a>2.8 Unix或POSIX时间戳：</h3><p>它是UNIX或类UNIX系统使用的时间表示方式。一般定义为从协调世界时(UTC时间)1970年1月1日0时0分0秒起至现在的总秒数（10位是精确到秒，13位是精确到毫秒）。考虑到闰秒的话，更精确的定义为<strong>从协调世界时(UTC时间)1970年1月1日0时0分0秒起至现在经过闰秒调整之后的总秒数</strong></p>]]></content>
      
      
      <categories>
          
          <category> GVINS </category>
          
          <category> NaN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GVINS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WeeklyReport-004-10Dec2023.md</title>
      <link href="/2023/12/10/WeeklyReport-004-10Dec2023-md/"/>
      <url>/2023/12/10/WeeklyReport-004-10Dec2023-md/</url>
      
        <content type="html"><![CDATA[<h1 id="工作周报"><a href="#工作周报" class="headerlink" title=" 工作周报 "></a><p style="text-align: center;"> 工作周报 </p></h1><p style="text-align: center;"> <b>时间: </b> 2023-12-04 ~ 2023-12-10 </p><br><h2 id="一、本周做了什么"><a href="#一、本周做了什么" class="headerlink" title="一、本周做了什么"></a>一、本周做了什么</h2><h3 id="1-训练GNSS中断场景下的LSTM网络"><a href="#1-训练GNSS中断场景下的LSTM网络" class="headerlink" title="1. 训练GNSS中断场景下的LSTM网络"></a>1. 训练GNSS中断场景下的LSTM网络</h3><p>使用sport_field数据集前5分钟数据进行训练和推理，其中前四分钟用于训练，后一分钟用于模型推理。</p><p><strong>第一种：端到端方案</strong><br>训练时：<br>模型输入为：经过IMU预积分后的位姿（P、V、Q），GVINS融合后的位姿（GNSS有效）<br>想让模型可以学习到这样的函数关系：GNSS有效下的融合位姿 &#x3D; f(IMU预积分后的位姿)<br>评价：这个思路是端到端的来看LSTM的效果，有点极端。GNSS失效时，仅仅依靠模型推理融合后的位姿。</p><p><strong>第二种：退化松耦合方案</strong><br>模型输入为：经过IMU预积分后的位姿（P、V、Q），GNSS有效情况时输出的经纬高（LLA）<br>想让模型可以学习到这样的函数关系：LLA &#x3D; f(IMU预积分后的位姿)</p><h3 id="2-构建LSTM效果评估方法"><a href="#2-构建LSTM效果评估方法" class="headerlink" title="2. 构建LSTM效果评估方法"></a>2. 构建LSTM效果评估方法</h3><p>评估采用的工具是evo，采用RTK在fix解下的LLA数据作为真值，仅评估三轴位置精度，，所有的结果均转换到东北天（ENU）坐标系下进行评价<br>评估指标采用业界常用的APE和RPE指标，关注各个指标的最大值、最小值、平均值和RMSE四个评价量<br>目前评估流程已经搭建完毕</p><p>轨迹界面：<br><img src="image-1.png" alt="Alt text"><br>东-北-天精度展示界面：<br><img src="image-2.png" alt="Alt text"><br>APE评估：<br><img src="image-3.png" alt="Alt text"><br>RPE评估：<br><img src="image-4.png" alt="Alt text"></p><h3 id="3-解决一些工程问题"><a href="#3-解决一些工程问题" class="headerlink" title="3. 解决一些工程问题"></a>3. 解决一些工程问题</h3><p><strong>3.1 ECEF-&gt; ENU, LLA-&gt;ENU转化函数的编写</strong><br>由于RTK真值数据的结果只有一个LLA数据，GVINS输出的是ECEF坐标系下数据，但是最终效果评估需要在ENU坐标系下进行，故需要编写ECEF-&gt; ENU, LLA-&gt;ENU函数。<br><strong>3.2 GNSS原始数据的保存</strong><br>因为下周想着让模型能学习GNSS原始数据与IMU之间的关系，所以需要想法子将GNSS原始数据保存下来。与IMU这种定长的数据不同，GNSS在每个时刻锁定的卫星数量是不确定的，所以不方便使用csv文件保存，故而采用json文件保存。我这种花了少量时间调研了C++常用的几种json文件读写工具，最终采用rapidjson实现卫星原始数据的保存，并学习了Rapidjson的用法，编写了相应的代码，目前已经完成了这部分功能的编写和测试，均工作正常。<br><strong>3.3 解决闰秒问题</strong><br>ROS端使用的是UTC时间，RTK使用的是GPS时间，GPS时间和UTC时间会存在一个闰秒问题，造成两者时间不同步，给评估工作带来麻烦</p><p>两者的转换关系是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UTC_time = GPS_time - err_sec</span><br></pre></td></tr></table></figure><p>其中，err_sec是一个时间差，主要由闰秒问题产生，如果各个时间完全精确的话，err_sec应该是18.0秒整，但是由于接收机的原因，两者肯定不会这么准，所以需要在线计算，现在是上电时锁定GNSS数据后就在线计算一个err_sec，目前这个值大概是17.9几左右，需要在评估的时候手动对RTK时间进行修正<br><img src="image.png" alt="Alt text"></p><p>此外，由于闰秒的存在，DT长度总会比GT短18秒左右。评估指令中需要注意</p><h2 id="二、下周怎么安排"><a href="#二、下周怎么安排" class="headerlink" title="二、下周怎么安排"></a>二、下周怎么安排</h2><ol><li>评估网络训练效果</li><li>结合文献、评估效果对网络参数、网络结构、网络输入输出进行优化</li><li>做完GNSS原始量测生成的实验，并评估效果</li></ol>]]></content>
      
      
      <categories>
          
          <category> NaN </category>
          
          <category> NaN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WeeklyReport-003-03Dec2023</title>
      <link href="/2023/12/03/WeeklyReport-003-03Dec2023/"/>
      <url>/2023/12/03/WeeklyReport-003-03Dec2023/</url>
      
        <content type="html"><![CDATA[<h2 id="一、本周做了什么"><a href="#一、本周做了什么" class="headerlink" title="一、本周做了什么"></a>一、本周做了什么</h2><h3 id="1-GVINS-代码功能增加"><a href="#1-GVINS-代码功能增加" class="headerlink" title="1. GVINS 代码功能增加"></a>1. GVINS 代码功能增加</h3><p>1.1 捋清GVINS代码逻辑<br>进一步研究了GVINS的代码逻辑，理清了其中的关键变量、关键函数，增加了详细的注释<br>在我的github主页可以看到<br><a href="mailto:&#x67;&#105;&#116;&#x40;&#x67;&#105;&#116;&#x68;&#117;&#98;&#46;&#99;&#x6f;&#x6d;">&#x67;&#105;&#116;&#x40;&#x67;&#105;&#116;&#x68;&#117;&#98;&#46;&#99;&#x6f;&#x6d;</a>:wxtcon&#x2F;gvins_comments_by_damon.git<br>（对GVINS的所有修改都会push到github上）</p><p>1.2 增加虚拟串口输出功能<br>虚拟串口主要是为了方便代码调试，可以通过虚拟串口在另一个终端打印调试信息，并通过终端输入调试参数</p><p>1.3 增加地图轨迹显示功能<br>地图轨迹显示主要是为了写论文的时候将真实轨迹和算法的推理轨迹放在叠加在google地图中（我看别人的论文都是这么做的）<br>我使用python写了一个脚本，实现了输入LLA在地图上显示轨迹的功能<br>代码在我的github上，路径为：GVINS&#x2F;script&#x2F;map_track_visual.py<br>效果如图所示：<br><img src="f9f4809365b12a726a137a320660f250.JPG" alt="Alt text"></p><h3 id="2-数据集生成"><a href="#2-数据集生成" class="headerlink" title="2. 数据集生成"></a>2. 数据集生成</h3><p>在GVINS中增加了保存数据的功能，主要包含IMU测量、image测量、GNSS测量，并写入GVINS文件</p><h3 id="3-NN方案设计"><a href="#3-NN方案设计" class="headerlink" title="3. NN方案设计"></a>3. NN方案设计</h3><p>设计了三种GNSS中断条件下的LSTM网络，具体的网络输入输出设计周六已经同师兄已经讨论过，目前正在加紧训练</p><h3 id="4-学习-充电"><a href="#4-学习-充电" class="headerlink" title="4. 学习&#x2F;充电"></a>4. 学习&#x2F;充电</h3><p>总结SLAM中各种优化的区别（附录）</p><h2 id="二、下周怎么安排"><a href="#二、下周怎么安排" class="headerlink" title="二、下周怎么安排"></a>二、下周怎么安排</h2><p>下周主要是对设计的几种网络进行训练，主要包含：</p><ol><li><p>复现 delta_P_GNSS &#x3D; f(IMU, T)网络<br>这部分已经在做了，但是目前还没搞出来评估网络输出delta_P的效果的方法，计划先把这个网络复现了<br>与原论文呢不同的是，我的这个网络考虑了GNSS失效时间T，原论文没考虑</p></li><li><p>复现 raw_measurement_GNSS &#x3D; f(IMU, visual, T)<br>这里的期望输出是卫星的原始量测</p></li><li><p>复现 LLA_GNSS &#x3D; f(IMU, visual, T)<br>这里的输出的LLA直接是经纬高</p></li><li><p>在训练的时候，设计一种GNSS输出效果的评估指标&#x2F;方式，用于评估网络的训练效果</p></li></ol><h2 id="附录：谈一谈对SLAM中何种“优化的理解”"><a href="#附录：谈一谈对SLAM中何种“优化的理解”" class="headerlink" title="附录：谈一谈对SLAM中何种“优化的理解”"></a>附录：谈一谈对SLAM中何种“优化的理解”</h2><h3 id="对优化的理解"><a href="#对优化的理解" class="headerlink" title="对优化的理解"></a>对优化的理解</h3><h4 id="非线性优化"><a href="#非线性优化" class="headerlink" title="非线性优化"></a>非线性优化</h4><p>从直观上来说，优化就是沿着梯度下降的方向去找到使误差(代价函数)最小化的优化变量。<br>非线性优化，自然就是误差函数和需要优化的变量之间不是简单的线性关系，而是非线性的。<br>常用的库有ceres。</p><h4 id="图优化"><a href="#图优化" class="headerlink" title="图优化"></a>图优化</h4><p>图优化的本质还是优化，只不过是用图的方式去表达。把优化变量当做顶点，约束当做边，也是用梯度下降的方法去使误差最小化。<br>图优化的好处：因为slam的特性，图是稀疏的，因此可以用图理论去进行边缘化，消元，加速计算。<br>常用的库有g2o。</p><h4 id="因子图优化"><a href="#因子图优化" class="headerlink" title="因子图优化"></a>因子图优化</h4><p>和图优化不同的是，它的重点在因子图。<br>它的优化落脚点不再是最小二乘理论了，而是最大后验概率理论了。</p><p>因子图优化可以在问题的拓扑结构上进行一些顶层抽象和简化，而不是直接硬解，相当于多了一步简化过程。<br>如果只有路标和位姿之间的因子，和BA优化完全一样。不过因子图是个大筐，什么约束都能加，IMU，轮速计，GPS。<br>边缘化掉节点之后还能保留稍微复杂一些的分布，信息损失较少。还有isam，可以在顶层结构上分析出新的测量值进来之后哪些节点需要优化，节省资源。</p><p>常用库有gtsam</p><p><strong>需要注意的是：</strong></p><ol><li>如果先验概率分布和观测噪声都是高斯分布，同时系统的动力学是线性的，那么MAP估计和最小二乘估计是等价的。<br>这是因为在这种情况下，MAP估计就是在最小二乘目标函数上加上一个正则化项，该正则化项对应于负对数先验概率。</li><li>当系统的观测模型是线性的时，MAP估计也等价于最小二乘估计。在这种情况下，MAP估计的最优解与最小二乘估计的解相同。</li><li>在非高斯、非线性的情况下，MAP估计可以通过使用数值优化方法来解决。这可能涉及到使用梯度下降、牛顿法、粒子滤波等技术。在这种情况下，MAP估计和最小二乘估计可能会产生不同的结果。</li><li>等价性的条件是有限制的，MAP估计更一般地适用于各种概率分布和模型。</li></ol><h4 id="位姿图优化"><a href="#位姿图优化" class="headerlink" title="位姿图优化"></a>位姿图优化</h4><p>位姿图优化，假设路标点是确定的，优化变量只有位姿，路标点成了约束。<br>位姿图是简化的BA<br>位姿图优化就是把各个位姿构成误差方程，优化各个位姿，不涉及路标点<br>位姿图需要考虑回环检测，最终矩阵可能不是稀疏矩阵。但好在不需要实时运行，cpu有空闲就算一些，有空闲就算一些，所以矩阵不稀疏也不是什么大问题。</p><h4 id="BA优化"><a href="#BA优化" class="headerlink" title="BA优化"></a>BA优化</h4><p>位姿图优化和BA优化都是slam中的具体问题<br>带有相机位姿和空间点的图优化称为BA<br>BA是SLAM和SFM定义出来的一个优化问题，以重投影误差为误差函数，以每个时刻位姿和所有路标点为优化变量<br>BA优化中，路标点和位姿都是不确定的，都是优化变量<br>BA优化，路标点数量远大于位姿数量，是一个箭头型矩阵，可以用舒尔补的方法转换成三角阵快速求解。在前端中，需要实时求解。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>本质都是非线性优化<br>因子图优化或者图优化指大规模稀疏的非线性优化，采用遍历图的方式去解非线性优化问题中的非线性方程组。<br>位姿图优化和ba优化是针对特定问题的图优化。<br>如果不去深究优化时选用什么样的矩阵分解方法的话，可以理解成都是一回事。<br>说白了，最后就是加权平均，没什么高深的。科技以改名为主。</p>]]></content>
      
      
      <categories>
          
          <category> NaN </category>
          
          <category> NaN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>周报-002-26Nov2023</title>
      <link href="/2023/11/26/WeeklyReport-002-26Nov2023/"/>
      <url>/2023/11/26/WeeklyReport-002-26Nov2023/</url>
      
        <content type="html"><![CDATA[<h1 id="工作周报"><a href="#工作周报" class="headerlink" title=" 工作周报 "></a><p style="text-align: center;"> 工作周报 </p></h1><p style="text-align: center;"> <b>时间: </b> 2023-11-20 ~ 2023-11-26 </p><br><h2 id="一、本周做了什么"><a href="#一、本周做了什么" class="headerlink" title="一、本周做了什么"></a>一、本周做了什么</h2><h3 id="1-pytorch扫盲"><a href="#1-pytorch扫盲" class="headerlink" title="1. pytorch扫盲"></a>1. pytorch扫盲</h3><p>学习pytorch的常用模块，并以CIFAR10编程实现一个图片分类网络以熟悉使用费pytorch进行NN搭建&amp;训练的流程<br>学习笔记：<a href="https://www.damonai.cn/2023/11/25/learn-pytorch/">https://www.damonai.cn/2023/11/25/learn-pytorch/</a><br>学习代码：github.com&#x2F;wxtcon&#x2F;learn_pytorch</p><h3 id="2-GVINS-dataset数据提取"><a href="#2-GVINS-dataset数据提取" class="headerlink" title="2. GVINS-dataset数据提取"></a>2. GVINS-dataset数据提取</h3><p>为了实现离线训练NN，需要将ros bag转为csv格式数据，写了一个脚本，实现rosbag按topic提取数据并写入csv中</p><h3 id="3-DL扫盲"><a href="#3-DL扫盲" class="headerlink" title="3. DL扫盲"></a>3. DL扫盲</h3><p>参考B站资料对Loss Fun、正则化、CNN、RNN、LSTM、GRU等知识点进行了扫盲</p><h2 id="二、下周怎么安排"><a href="#二、下周怎么安排" class="headerlink" title="二、下周怎么安排"></a>二、下周怎么安排</h2><ol><li>写一个提取GVINS误差的代码<br>原始的GVINS-dataset中只有星历、伪距这些数据，不能进行网络的训练，无法直接在GNSS拒止状态下输出GVINS误差，需要先跑一遍GVINS代码，把各个传感器的误差项提取出来</li><li>用GNSS误差数据对LSTM—NN进行离线训练，并评估效果</li></ol>]]></content>
      
      
      <categories>
          
          <category> NaN </category>
          
          <category> NaN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch入门笔记</title>
      <link href="/2023/11/25/learn-pytorch/"/>
      <url>/2023/11/25/learn-pytorch/</url>
      
        <content type="html"><![CDATA[<h1 id="Pytorch入门笔记"><a href="#Pytorch入门笔记" class="headerlink" title="Pytorch入门笔记"></a>Pytorch入门笔记</h1><p><strong>参考资源：bilibili - 我是土堆</strong><br>url : <a href="https://www.bilibili.com/video/BV1hE411t7RN/?spm_id_from=333.999.0.0&vd_source=9761a01d2f08a7425632b3ad97cccf18">https://www.bilibili.com/video/BV1hE411t7RN/?spm_id_from=333.999.0.0&amp;vd_source=9761a01d2f08a7425632b3ad97cccf18</a></p><h1 id="一、-数据加载与预处理"><a href="#一、-数据加载与预处理" class="headerlink" title="一、 数据加载与预处理"></a>一、 数据加载与预处理</h1><h2 id="1-pytorch加载数据"><a href="#1-pytorch加载数据" class="headerlink" title="1. pytorch加载数据"></a>1. pytorch加载数据</h2><p><strong>Dataset的使用</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.data import Dataset</span><br><span class="line">from PIL import Image</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">class MyData(Dataset):</span><br><span class="line"></span><br><span class="line">    def __init__(self, root_dir, label_dir):</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.label_dir = label_dir</span><br><span class="line">        self.path = os.path.join(self.root_dir, self.label_dir)</span><br><span class="line">        self.img_path = os.listdir(self.path)</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        img_name = self.img_path[index]</span><br><span class="line">        img_item_path = os.path.join(self.root_dir, self.label_dir, img_name)</span><br><span class="line">        img = Image.open(img_item_path)</span><br><span class="line">        label = self.label_dir</span><br><span class="line"></span><br><span class="line">        return img, label</span><br><span class="line">    </span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.img_path)</span><br><span class="line">    </span><br><span class="line">root_dir = &#x27;dataset/Type1/train&#x27;</span><br><span class="line">ants_label_dir = &#x27;ants&#x27;</span><br><span class="line">bees_label_dir = &#x27;bees&#x27;</span><br><span class="line"></span><br><span class="line">ants_dataset = MyData(root_dir, ants_label_dir)</span><br><span class="line">bees_dataset = MyData(root_dir, bees_label_dir)</span><br><span class="line"></span><br><span class="line">train_dataset = ants_dataset + bees_dataset</span><br><span class="line"></span><br><span class="line">img, label = train_dataset[0]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="2-TensorBoard的使用"><a href="#2-TensorBoard的使用" class="headerlink" title="2. TensorBoard的使用"></a>2. TensorBoard的使用</h2><p>用于训练模型的时候显示图像（LOSS曲线）等</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line">import numpy as np</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(&quot;logs&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">img_path = &#x27;dataset/Type1/train/ants/6240329_72c01e663e.jpg&#x27;</span><br><span class="line">img = Image.open(img_path)</span><br><span class="line">img_array = np.array(img)</span><br><span class="line"></span><br><span class="line">writer.add_image(&#x27;test&#x27;, img_array, 1, dataformats=&#x27;HWC&#x27;) # 显示图像</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># for i in range(100):</span><br><span class="line">#     writer.add_scalar(&#x27;y = x&#x27;, 2*i, i) # 显示标量数据</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># writer.add_scalar()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>在终端输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=logs</span><br></pre></td></tr></table></figure><p>在浏览器中即可看到显示的图像</p><h2 id="3-Transforms的使用"><a href="#3-Transforms的使用" class="headerlink" title="3. Transforms的使用"></a>3. Transforms的使用</h2><p>Transforms主要是对图像进行一些变换</p><p><strong>a. Transforms 应该如何使用</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from PIL import Image</span><br><span class="line">from torchvision import transforms</span><br><span class="line"></span><br><span class="line">img_path = &quot;dataset/Type1/train/ants/6743948_2b8c096dda.jpg&quot;</span><br><span class="line"></span><br><span class="line">img = Image.open(img_path)</span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor() # 对象实例化</span><br><span class="line">tensor_img = tensor_trans(img)</span><br><span class="line"></span><br><span class="line">print(tensor_img)</span><br></pre></td></tr></table></figure><p><strong>b. 为什么需要使用tensor的数据类型</strong></p><p>tensor可以简单粗暴的理解为将图片等传统的数据封装为神经网络所需的数据类型</p><p><strong>c. 常见的transforms</strong></p><ol><li><p>Compose<br> compose()中的参数需要是一个列表，且列表中的数据需要时transforms类型，即：<br> compose([tansforms参数1， tansforms参数2])</p><p> 在我理解compose是将多个图像操作合起来了，例如将图像先裁剪再转换为tensor格式<br> 【tansforms参数1】的输出需要时【tansforms参数2】的输入，注意格式需要满足衔接需求</p></li><li><p>ToTensor<br> 将PIL或者opencv的数据格式转换为tensor的格式</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor_trans = transforms.ToTensor() </span><br><span class="line">tensor_img = tensor_trans(img)</span><br></pre></td></tr></table></figure></li><li><p>Normalize<br> 归一化</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trans_norm = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) # mean std</span><br><span class="line">img_norm = trans_norm(tensor_img)</span><br></pre></td></tr></table></figure></li><li><p>Resize<br>trans_resize &#x3D; transforms.Resize((128, 128))<br>or<br>trans_resize &#x3D; transforms.Resize((128))<br>img_resize &#x3D; trans_resize(img)</p></li><li><p>RandomCrop<br>用法同Resize</p></li></ol><h2 id="4-torchvision中的Dataset使用"><a href="#4-torchvision中的Dataset使用" class="headerlink" title="4. torchvision中的Dataset使用"></a>4. torchvision中的Dataset使用</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset_transform = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=&quot;./my_dataset&quot;,</span><br><span class="line">                                         train=True,</span><br><span class="line">                                         download=True,</span><br><span class="line">                                         transform=dataset_transform)</span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=&quot;./my_dataset&quot;,</span><br><span class="line">                                        train=False,</span><br><span class="line">                                        download=True,</span><br><span class="line">                                        transform=dataset_transform)</span><br><span class="line"></span><br><span class="line"># dataset_transforms = torchvision.transforms.Compose([])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># img.show()</span><br><span class="line">writer = SummaryWriter(&#x27;pic10&#x27;)</span><br><span class="line"></span><br><span class="line">print(test_set[0])</span><br><span class="line"></span><br><span class="line">for i in range(10):</span><br><span class="line">    img, target = test_set[i]</span><br><span class="line">    writer.add_image(&#x27;pic&#x27;, img, i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h2 id="5-DataLoader的使用"><a href="#5-DataLoader的使用" class="headerlink" title="5. DataLoader的使用"></a>5. DataLoader的使用</h2><p>DataLoader参数说明：<br>– dataset：打包好的tensor格式的数据<br>– batch_size： 一轮抓数据的个数（每一轮抓图片的张数）<br>– shuffle： 每个epoch结束后是否打乱所有图片（True为打乱，False为不打乱）<br>– num_workers: 多线程运行数（在windows下运行时此变量若大于1可能会出错）<br>– drop_last：最后一组数据不够一个batch_size数量时，是否丢弃，True为丢弃，False为不丢弃</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset_transform = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(&quot;./my_dataset&quot;, train=False, transform=dataset_transform, download=True)</span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=128, shuffle=True, num_workers=0, drop_last=False)</span><br><span class="line"></span><br><span class="line">img, target = test_data[0]</span><br><span class="line">print(img.shape)</span><br><span class="line">print(target)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(&#x27;dataloader&#x27;)</span><br><span class="line"></span><br><span class="line">step = 0</span><br><span class="line">for data in test_loader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    # print(imgs.shape)</span><br><span class="line">    # print(targets)</span><br><span class="line">    writer.add_images(&quot;dataloader&quot;, imgs, step)</span><br><span class="line">    step = step + 1</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="二、神经网络"><a href="#二、神经网络" class="headerlink" title="二、神经网络"></a>二、神经网络</h1><h2 id="6-神经网络基本骨架nn-Module的使用"><a href="#6-神经网络基本骨架nn-Module的使用" class="headerlink" title="6. 神经网络基本骨架nn.Module的使用"></a>6. 神经网络基本骨架nn.Module的使用</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">class Damon(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__() # 调用父类的方法</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = input + 1</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">damon = Damon()</span><br><span class="line">x = torch.tensor(1.0)</span><br><span class="line">output = damon(x)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure><h2 id="7-卷积操作"><a href="#7-卷积操作" class="headerlink" title="7. 卷积操作"></a>7. 卷积操作</h2><ol><li>torch.nn是对torch.nn.function的封装，一般会torch.nn即可</li><li>conv2d()函数操作<br> stride：步进，即每次卷积核移动的距离<br> padding：图片边缘掩膜填充</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">input = torch.tensor([[1, 2, 0, 3, 1],</span><br><span class="line">                      [0, 1, 2, 3, 1],</span><br><span class="line">                      [1, 2, 1, 0, 0],</span><br><span class="line">                      [5, 2, 3, 1, 1],</span><br><span class="line">                      [2, 1, 0, 1, 1]])</span><br><span class="line"></span><br><span class="line">kernel = torch.tensor([[1, 2, 1],</span><br><span class="line">                       [0, 1, 0],</span><br><span class="line">                       [2, 1, 0]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">input = torch.reshape(input, (1, 1, input.shape[0], input.shape[1]))</span><br><span class="line">kernel = torch.reshape(kernel, (1, 1, kernel.shape[0], kernel.shape[1]))</span><br><span class="line"></span><br><span class="line">output = F.conv2d(input, kernel, stride=1, padding=1)</span><br><span class="line"></span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure><h2 id="8-卷积层"><a href="#8-卷积层" class="headerlink" title="8. 卷积层"></a>8. 卷积层</h2><p>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride&#x3D;1, padding&#x3D;0, dilation&#x3D;1, groups&#x3D;1, bias&#x3D;True, padding_mode&#x3D;’zeros’, device&#x3D;None, dtype&#x3D;None)</p><p>– in_channels： 输入图像的通道数，一般彩色图像都是3<br>– out_channels： 输出通道数<br>– kernel_size ： 卷积核大小，例如3（即3<em>3的卷积核） （1，2）（即1</em>2的卷积核，不规则卷积核），定义的时候只需要设定卷积核大小，卷积核的值会自动采样得到<br>– stride ： 卷积核步进<br>– padding：边缘填充<br>– padding_mode&#x3D;’zeros’： 以0填充<br>– bias：偏置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import Conv2d</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(&quot;./my_dataset&quot;,</span><br><span class="line">                                       train=False,</span><br><span class="line">                                       transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=True)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=64)</span><br><span class="line"></span><br><span class="line">class Damon(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Damon, self).__init__()</span><br><span class="line">        self.conv1 = Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=0)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">damon = Damon()</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(&#x27;./logs&#x27;)</span><br><span class="line"></span><br><span class="line">step = 0</span><br><span class="line">for data in dataloader:</span><br><span class="line">    step = step + 1</span><br><span class="line">    imgs, targets = data</span><br><span class="line"></span><br><span class="line">    output = damon(imgs)</span><br><span class="line">    # print(output.shape)</span><br><span class="line">    writer.add_images(&#x27;input&#x27;, imgs, step)</span><br><span class="line"></span><br><span class="line">    # output = torch.reshape(output, (-1, 3, 30, 30))</span><br><span class="line">    # output = output.view(-1, 3, 30, 30)</span><br><span class="line">    writer.add_images(&#x27;output&#x27;, output, step)</span><br></pre></td></tr></table></figure><h2 id="9-池化层"><a href="#9-池化层" class="headerlink" title="9. 池化层"></a>9. 池化层</h2><p>最大池化又称下采样<br>最大池化的作用是：保持输入数据的特征，同时减小数据量，网络参数会减少，训练速度会更快<br>所以在很多网络中，大家卷积完之后都会来一次最大池化，然后再来一次非线性激活</p><p>torch.nn.MaxPool2d(kernel_size, stride&#x3D;None, padding&#x3D;0, dilation&#x3D;1, return_indices&#x3D;False, ceil_mode&#x3D;False)<br>参数：<br>– kernel_size (Union[int, Tuple[int, int]]) – the size of the window to take a max over</p><p>– stride (Union[int, Tuple[int, int]]) – the stride of the window. Default value is <strong>kernel_size</strong> （区别卷积层的default是1）</p><p>– padding (Union[int, Tuple[int, int]]) – Implicit negative infinity padding to be added on both sides</p><p>– dilation (Union[int, Tuple[int, int]]) – a parameter that controls the stride of elements in the window （设置空洞卷积）</p><p>– return_indices (bool) – if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later</p><p>– ceil_mode (bool) – when True, will use ceil instead of floor to compute the output shape （default &#x3D; false）<br>ceil和floor的区别看下图</p><p><img src="image.png" alt="Alt text"><br>ceil和floor的具体区别直观展示：<br><img src="image-1.png" alt="Alt text"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import MaxPool2d</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(root=&#x27;./my_dataset&#x27;,</span><br><span class="line">                                       train=False,</span><br><span class="line">                                       transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=True)</span><br><span class="line">dataload = DataLoader(dataset, batch_size=64)</span><br><span class="line"></span><br><span class="line"># input = torch.tensor([[1, 2, 0, 3, 1],</span><br><span class="line">#               [0, 1, 2, 3, 1],</span><br><span class="line">#               [1, 2, 1, 0, 0],</span><br><span class="line">#               [5, 2, 3, 1, 1],</span><br><span class="line">#               [2, 1, 0, 1, 1]], dtype=torch.float32)</span><br><span class="line"># input = torch.reshape(input, (-1, 1, 5, 5))</span><br><span class="line"></span><br><span class="line">class Damon(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Damon, self).__init__()</span><br><span class="line">        self.maxpool1 = MaxPool2d(kernel_size=3, ceil_mode=False)</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = self.maxpool1(input)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">damon = Damon();</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(&#x27;logs&#x27;)</span><br><span class="line">step = 0</span><br><span class="line">for data in dataload:</span><br><span class="line">    step = step + 1</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = damon(imgs)</span><br><span class="line">    writer.add_images(&#x27;maxpool&#x27;, output, step)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="10-padding层"><a href="#10-padding层" class="headerlink" title="10. padding层"></a>10. padding层</h2><p>对输入图像周围进行填充<br>一般用不到，因为一般在conv中我们会通过padding参数进行填充</p><p>eg：nn.ZeroPad2d</p><ul><li>Pads the input tensor boundaries with zero.</li></ul><h2 id="11-非线性激活"><a href="#11-非线性激活" class="headerlink" title="11. 非线性激活"></a>11. 非线性激活</h2><p>非线性变换的主要目的就是为了给网络中引入一些非线性特征<br>因为非线性特征越多的话，模型泛化能力越好</p><p>eg:<br><strong>–ReLu:</strong><br>小于0时进行阶段，大于0时原样输出<br><img src="image-2.png" alt="Alt text"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import ReLU</span><br><span class="line"></span><br><span class="line">input = torch.tensor([[1, -0.5],</span><br><span class="line">             [-0.2, 3]])</span><br><span class="line"></span><br><span class="line">class Damon(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Damon, self).__init__()</span><br><span class="line">        self.relu1 = ReLU()</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = self.relu1(input)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">damon = Damon()</span><br><span class="line">output = damon(input)</span><br><span class="line"></span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure><p><strong>– sigmoid</strong><br><img src="image-3.png" alt="Alt text"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import Sigmoid</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(root=&#x27;./my_dataset&#x27;,</span><br><span class="line">                                       train=False,</span><br><span class="line">                                       transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=True)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=64)</span><br><span class="line"></span><br><span class="line">class Damon(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Damon, self).__init__()</span><br><span class="line">        self.sigmoid1 = Sigmoid()</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = self.sigmoid1(input)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">damon = Damon()</span><br><span class="line">writer = SummaryWriter(&#x27;./logs&#x27;)</span><br><span class="line">step = 0</span><br><span class="line">for data in dataloader:</span><br><span class="line">    step = step + 1</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = damon(imgs)</span><br><span class="line">    writer.add_images(&#x27;sigmoid&#x27;, output, step)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h2 id="12-线性层及其它层"><a href="#12-线性层及其它层" class="headerlink" title="12. 线性层及其它层"></a>12. 线性层及其它层</h2><ol><li>正则化层<br>采用正则化层可以加快网络的训练速度<br>用的比较少</li><li>Recurrent Layers<br>RNN &#x2F;LSTM etc<br>属于特定的网络结构</li><li>Transformer Layer</li><li>线性层<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import Linear</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(&quot;./my_dataset&quot;,</span><br><span class="line">                                       train=False,</span><br><span class="line">                                       transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=True)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=64)</span><br><span class="line"></span><br><span class="line">class Damon(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Damon, self).__init__()</span><br><span class="line">        self.linear1 = Linear(196608,10)</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = self.linear1(input)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">t = Damon()</span><br><span class="line">for data in dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    imgs = torch.flatten(imgs)</span><br><span class="line">    output = t(imgs)</span><br><span class="line">    print(output.shape)</span><br></pre></td></tr></table></figure></li><li>Dropout Layer<br>为了防止模型过拟合</li><li>Flatten 展平<br>from torch.nn import Flatten<br>self.flatten &#x3D; Flatten()</li></ol><h2 id="13-NN-搭建实战和Sequential的使用"><a href="#13-NN-搭建实战和Sequential的使用" class="headerlink" title="13. NN 搭建实战和Sequential的使用"></a>13. NN 搭建实战和Sequential的使用</h2><p><strong>a. Sequential的使用</strong><br>Sequential类似于compose,是将多个操作（层）合起来的一个工具</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">          nn.Conv2d(1,20,5),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(20,64,5),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">          (&#x27;conv1&#x27;, nn.Conv2d(1,20,5)),</span><br><span class="line">          (&#x27;relu1&#x27;, nn.ReLU()),</span><br><span class="line">          (&#x27;conv2&#x27;, nn.Conv2d(20,64,5)),</span><br><span class="line">          (&#x27;relu2&#x27;, nn.ReLU())</span><br><span class="line">        ]))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>b. CIFAR10 Network_model</strong><br><img src="image-4.png" alt="Alt text"></p><p>Tips：使用TensorBoard中的add_graph可以将网络展开成如下形状，便于直观检查网络的问题</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writer = SummaryWriter(&#x27;./logs&#x27;)</span><br><span class="line">writer.add_graph(damon, input)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p><img src="image-5.png" alt="Alt text"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">class Damon(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Damon, self).__init__()</span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(3, 32, 5, padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Conv2d(32, 32, 5, padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Conv2d(32, 64, 5, padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(1024, 64),</span><br><span class="line">            Linear(64, 10)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = self.model1(input)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">damon = Damon()</span><br><span class="line"></span><br><span class="line">input = torch.ones((64, 3, 32, 32))</span><br><span class="line">output = damon(input)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(&#x27;./logs&#x27;)</span><br><span class="line">writer.add_graph(damon, input)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="三、-pytorch应用"><a href="#三、-pytorch应用" class="headerlink" title="三、 pytorch应用"></a>三、 pytorch应用</h1><h2 id="14-损失函数与反向传播"><a href="#14-损失函数与反向传播" class="headerlink" title="14. 损失函数与反向传播"></a>14. 损失函数与反向传播</h2><p>a. Loss Fun的作用<br>    1.计算实际输出和实际目标之间的差距<br>    2.为参数更新提供依据（反向传播）<br>    实际上神经网络的参数就是卷积核的参数，参数更新就是根据loss计算参数的梯度，以便进行梯度下降，达到Loss下降的目的<br>b. 常用的LOSS函数<br>    L1Loss<br>    MSELoss<br>    CrossEntropyLoss 交叉熵损失 分类中常用<br><img src="image-6.png" alt="Alt text"><br>c. 反向传播代码<br>反向传播会求出每一个参数的梯度</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for data in dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = damon(imgs)</span><br><span class="line">    print(output)</span><br><span class="line">    print(&#x27;target:&#x27;, targets)</span><br><span class="line">    loss = celoss(output, targets)</span><br><span class="line">    loss.backward()</span><br><span class="line">    print(&#x27;loss:&#x27;, loss)</span><br><span class="line">    exit()</span><br></pre></td></tr></table></figure><h2 id="15-优化器"><a href="#15-优化器" class="headerlink" title="15. 优化器"></a>15. 优化器</h2><p><strong>a. 优化器的使用套路</strong></p><p>定义一个优化器，设置模型参数和学习率等参数<br><code>optim = torch.optim.SGD(damon.parameters(), lr=0.01)</code><br>↓<br>在每个batch_size中,先清零梯度，因为上一次的梯度对于本次的训练是没用的<br><code>optim.zero_grad()</code><br>↓<br>loss反向传播<br><code>loss.backward()</code><br>↓<br>执行优化,即：模型参数调优<br><code>optim.step()</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">damon = Damon() # instantiation</span><br><span class="line">celoss = nn.CrossEntropyLoss() # Loss definition</span><br><span class="line">optim = torch.optim.SGD(damon.parameters(), lr=0.01)</span><br><span class="line"></span><br><span class="line">for epoch in range(20):</span><br><span class="line">    run_loss = 0.0</span><br><span class="line">    for data in dataloader:</span><br><span class="line">        optim.zero_grad()</span><br><span class="line"></span><br><span class="line">        imgs, targets = data</span><br><span class="line">        output = damon(imgs)</span><br><span class="line"></span><br><span class="line">        loss = celoss(output, targets)</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        optim.step()</span><br><span class="line">        run_loss = run_loss + loss</span><br><span class="line">    print(run_loss)</span><br></pre></td></tr></table></figure><p><strong>b. 学习率</strong></p><p>学习速率太大，模型训练起来就很不稳定<br>学习速率太小，模型训练会很慢</p><p>一般先用大的学习速率，再用小的学习速率</p><h2 id="16-现有模型的加载使用与修改"><a href="#16-现有模型的加载使用与修改" class="headerlink" title="16. 现有模型的加载使用与修改"></a>16. 现有模型的加载使用与修改</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vgg16_raw = torchvision.models.vgg16() # 加载VGG16</span><br><span class="line"></span><br><span class="line">vgg16_raw.add_module(&#x27;my_linear&#x27;, nn.Linear(1000, 10)) # 在vgg16后面加线性层</span><br><span class="line">vgg16_raw.classifier.add_module(&quot;my_linear2&quot;, nn.Linear(100, 100)) # 在vgg16的classifier后面加线性层</span><br><span class="line">vgg16_raw.classifier[6] = nn.Linear(4096, 10) # 修改vgg16的classifier的6号网络层为Linear(4096, 10)</span><br></pre></td></tr></table></figure><h2 id="17-网络模型的保存与读取"><a href="#17-网络模型的保存与读取" class="headerlink" title="17. 网络模型的保存与读取"></a>17. 网络模型的保存与读取</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># raw method</span><br><span class="line">vgg16 = torchvision.models.vgg16()</span><br><span class="line"></span><br><span class="line">**# save method1:**</span><br><span class="line">torch.save(vgg16, &quot;vgg16_method1.pth&quot;)</span><br><span class="line"></span><br><span class="line"># load method1:</span><br><span class="line">model1 = torch.load(&quot;vgg16_method1.pth&quot;)</span><br><span class="line"># -------------------------</span><br><span class="line"></span><br><span class="line">**# save method2:** （官方推荐的方式）</span><br><span class="line">torch.save(vgg16.state_dict(), &#x27;vgg16_method2.pth&#x27;)</span><br><span class="line"></span><br><span class="line"># load method2:</span><br><span class="line">model2 = torch.load(&quot;vgg16_method2.pth&quot;) # only load model parameter, not contains the construction of the model</span><br><span class="line">vgg16.load_state_dict(torch.load(&quot;vgg16_method2.pth&quot;))</span><br></pre></td></tr></table></figure><h2 id="18-完成的模型训练套路（以CIFAR10为例）"><a href="#18-完成的模型训练套路（以CIFAR10为例）" class="headerlink" title="18. 完成的模型训练套路（以CIFAR10为例）"></a>18. 完成的模型训练套路（以CIFAR10为例）</h2><p>a. 使用正确率评估模型（分类问题中特有的评价指标）<br>意思就是：测试集中分类正确的图片占比多少？</p><p>完整的代码在github的project子文件夹下 train.py, 模型在model.py里</p><p>b. 训练模型的一些细节</p><ol><li>有的人在训练前会加damon.train()&#x2F;在验证前加damon.eval()等字样，这个不是必须的，这个代码只对某些层有效（例如Dropout、BatchNorm等），但是如果自己的网络里有这些特殊的层，就必须在训练&#x2F;测试前调用这些代码</li></ol><h2 id="19-使用GPU进行训练"><a href="#19-使用GPU进行训练" class="headerlink" title="19. 使用GPU进行训练"></a>19. 使用GPU进行训练</h2><p>a. 第一种方法<br>找到【模型】、【数据（输入+标注）】、【损失函数】，在这三个后面加一句.cuda()<br>eg:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">damon = Damon()</span><br><span class="line">damon = damon.cuda()</span><br></pre></td></tr></table></figure><p>b. 第二种方法<br>使用.todevice()方法</p><p>device &#x3D; torch.device(‘cpu’) # 使用CPU<br>         torch.device(‘cuda’) # 使用GPU<br>         torch.device(‘cuda:0’) # 有多张显卡时，使用第一张GPU<br>         torch.device(‘cuda:1’) # 有多张显卡时，使用第2张GPU</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&#x27;cpu&#x27;)</span><br><span class="line"></span><br><span class="line">damon = Damon()</span><br><span class="line">damon = damon.to(device)</span><br></pre></td></tr></table></figure><p>更常见的一种方式：<br><code>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</code></p><p>c. 如果自己没有GPU可以去谷歌的colab蹭</p><h2 id="18-完成的模型测试套路"><a href="#18-完成的模型测试套路" class="headerlink" title="18. 完成的模型测试套路"></a>18. 完成的模型测试套路</h2><p>核心：利用已经训练好的模型吗，给他提供输入，进行模型推理</p><p>步骤：找测试数据（eg：图片）-&gt; reshape到模型接受的大小 -&gt; 输入模型推理</p><p>Attention: 如果模型实在GPU上训练的，那么使用CPU推理，在加载时需要制定map_location，即：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.load(&#x27;model.pth&#x27;, map_location=torch.device(&#x27;cpu&#x27;))</span><br></pre></td></tr></table></figure><h3 id="19-看看开源项目"><a href="#19-看看开源项目" class="headerlink" title="19. 看看开源项目"></a>19. 看看开源项目</h3><p>1. </p>]]></content>
      
      
      <categories>
          
          <category> NaN </category>
          
          <category> NaN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>周报-001-19Nov2023</title>
      <link href="/2023/11/18/WeeklyReport-001-19Nov2023/"/>
      <url>/2023/11/18/WeeklyReport-001-19Nov2023/</url>
      
        <content type="html"><![CDATA[<h1 id="工作周报"><a href="#工作周报" class="headerlink" title=" 工作周报 "></a><p style="text-align: center;"> 工作周报 </p></h1><p style="text-align: center;"> <b>时间: </b> 2023-11-13 ~ 2023-11-18 </p><br><h2 id="一、本周做了什么"><a href="#一、本周做了什么" class="headerlink" title="一、本周做了什么"></a>一、本周做了什么</h2><p>本周主要同步推进paper和硬件小车这两件事，主要有：</p><ol><li>Paper Reading：在 <strong>GNSS-Visual-Inertial-odometry(GVIO)</strong> 领域阅读文献多篇，其中精读3篇，并完成Reading Note（纸质），</li><li>Paper Writing: 完成了<strong>论文的（通用）部分</strong>，主要包含：<strong>坐标系的转换描述</strong>、<strong>因子图原理</strong>、<strong>Introduction部分的骨架</strong>等。</li><li>Code  Reading: 捋完了<strong>GVINS</strong>的框架，将其作为Code-Baseline在这上面修改。</li><li>学习LSTM，并基于LSTM完成了一个股票数据的预测，算作入门。</li><li>调校小车：摸清了 <strong>传感器数据采集→STM32数据收集和转发→PC机（ubuntu+ROS）</strong>的全流程；将计算任务转移到<strong>个人PC</strong>上运行；在线运行ORB-SLAM2纯视觉建图<strong>效果一般但不卡</strong>；将小车部分的启动步骤、参数细节整理成了文档。</li><li>制作做CAC会议海报，来重庆开会。</li></ol><h2 id="二、有什么收获-启发-问题"><a href="#二、有什么收获-启发-问题" class="headerlink" title="二、有什么收获&#x2F;启发&#x2F;问题"></a>二、有什么收获&#x2F;启发&#x2F;问题</h2><h3 id="2-1-Paper-Reading-GVIO"><a href="#2-1-Paper-Reading-GVIO" class="headerlink" title="2.1 Paper Reading - GVIO"></a>2.1 Paper Reading - GVIO</h3><p>  详情在纸质版笔记上，供论文的Related Work部分使用。</p><h3 id="2-2-Code-Reading-Writting-GVINS"><a href="#2-2-Code-Reading-Writting-GVINS" class="headerlink" title="2.2 Code  Reading &amp; Writting - GVINS"></a>2.2 Code  Reading &amp; Writting - GVINS</h3><ol><li>我详细阅读并注释了GVINS的代码，并已经push到我的github，链接：<a href="https://github.com/wxtcon/gvins_comments_by_damon">https://github.com/wxtcon/gvins_comments_by_damon</a></li><li>编写选星这部分算法代码，基于GVINS-Dataset测试，这会还没写完，预计下周写完并测试</li></ol><h3 id="2-3-调校小车"><a href="#2-3-调校小车" class="headerlink" title="2.3 调校小车"></a>2.3 调校小车</h3><p><strong>工作细节</strong></p><ol><li><p>尚未定位为什么在Jetson TX2上ORB-SLAM3非常卡（<strong>一核有难，三核围观</strong>）的原因，这个问题定位起来有点麻烦（<strong>暂缓</strong>）</p></li><li><p>鉴于问题1.难定位，我将工控机的所有任务转移到<strong>个人PC</strong>上进行，目前<strong>全流程已经打通</strong>（包括环境配置、代码编译测试、运行测试），现阶段小车长这样<br><img src="IMG_20231115_150317.jpg" alt="Alt text"><br><img src="IMG_20231115_150312.jpg" alt="Alt text"></p></li><li><p>使用<strong>camera-calibration</strong>工具完成了摄像头的标定，标定工具界面如下:<br><img src="cailb.bmp" alt="Alt text"></p></li><li><p>在PC上编译并运行ORB-SLAM2，在<strong>会议室</strong>进行纯视觉建图测试，<br><img src="1.bmp" alt="Alt text"><br>体素地图效果如下：<br>  <img src="2.bmp" alt="Alt text"><br>  可以发现下面这块地图发生了较大的偏移，这是因为这块地方都是墙壁，特征点太少了，发生了退化</p></li><li><p>将上述工作整理成了一个简单的文档备查<br>文档链接：<a href="http://www.damonai.cn/2023/11/15/Car-Info/">www.damonai.cn/2023/11/15/Car-Info/</a></p></li></ol><h3 id="2-4-LSTM学习"><a href="#2-4-LSTM学习" class="headerlink" title="2.4 LSTM学习"></a>2.4 LSTM学习</h3><p>学习LSTM原理，在服务器上跑了一个预测股票的demo，<br><img src="image.png" alt="Alt text"><br>之前没怎么搞过深度学习，正在补基础知识</p><h2 id="三、下阶段怎么安排"><a href="#三、下阶段怎么安排" class="headerlink" title="三、下阶段怎么安排"></a>三、下阶段怎么安排</h2><ol><li>计划用两天左右时间完成深度学习基础+RNN+LSTM扫盲</li><li>完成卫星选星算法代码的编写和效果测试（基于GVINS-Dataset）。</li><li>完成小车硬件升级：小车加高，先加一层50cm（200&#x2F;层，已买），后续视情况判断是否再加一层(凑1m)<br><img src="Screenshot_2023-11-15-23-25-47-030_com.taobao.tao.jpg" alt="Alt text"></li></ol><p><br><br></p>]]></content>
      
      
      <categories>
          
          <category> NaN </category>
          
          <category> NaN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Car-Info</title>
      <link href="/2023/11/15/Car-Info/"/>
      <url>/2023/11/15/Car-Info/</url>
      
        <content type="html"><![CDATA[<h1 id="小车信息"><a href="#小车信息" class="headerlink" title=" 小车信息 "></a><p style="text-align: center;"> 小车信息 </p></h1><h2 id="一、常用配置命令"><a href="#一、常用配置命令" class="headerlink" title="一、常用配置命令"></a>一、常用配置命令</h2><ol><li><p>在工作空间下运行，安装ROS功能包全部依赖（镜像中已配置rosdep）：<br>rosdep install –from-paths src –ignore-src -r -y</p></li><li><p>指定功能包编译：<br>catkin_make -DCATKIN_WHITELIST_PACKAGES&#x3D;”功能包名”<br>解除指定功能包编译：<br>catkin_make -DCATKIN_WHITELIST_PACKAGES&#x3D;””</p></li><li><p>打开摄像头并使用rqt工具查看图像话题：<br>roslaunch turn_on_wheeltec_robot wheeltec_camera.launch<br>rqt_image_view</p></li><li><p>生成TF树pdf<br>rosrun tf view_frames</p></li><li><p>查看TF树<br>rosrun rqt_tf_tree rqt_tf_tree</p></li><li><p>nfs挂载:<br>sudo mount -t nfs 192.168.0.100:&#x2F;home&#x2F;wheeltec&#x2F;wheeltec_robot &#x2F;mnt<br>nfs解除挂载:<br>sudo umount -t nfs 192.168.0.100:&#x2F;home&#x2F;wheeltec&#x2F;wheeltec_robot &#x2F;mnt</p></li></ol><h2 id="二、常用功能命令"><a href="#二、常用功能命令" class="headerlink" title="二、常用功能命令"></a>二、常用功能命令</h2><p><strong>1.开启初始化节点</strong><br>（仅在单独开启键盘控制时需要开启 运行功能时已包括初始化节点 不需要重复开启）<br>roslaunch turn_on_wheeltec_robot turn_on_wheeltec_robot.launch<br>&#x2F;&#x2F;开启键盘控制节点<br>roslaunch wheeltec_robot_rc keyboard_teleop.launch</p><p><strong>2.ORB-SLAM2建图</strong><br>&#x2F;&#x2F;开启ORB节点<br>roslaunch turn_on_wheeltec_robot orb_slam.launch<br>&#x2F;&#x2F;开启rviz可视化工具<br>rviz -d robot_ws&#x2F;src&#x2F;orb_slam2_ros-master&#x2F;ros&#x2F;launch&#x2F;orb_slam.rviz<br>（rviz配置文件路径：orb_slam2_ros-master&#x2F;ros&#x2F;launch&#x2F;orb_slam.rviz）<br>&#x2F;&#x2F; 手动保存地图(在地图要保存路径下运行)：<br>rosrun map_server map_saver -f </p><h2 id="三、Astra-Pro相机内参数"><a href="#三、Astra-Pro相机内参数" class="headerlink" title="三、Astra Pro相机内参数"></a>三、Astra Pro相机内参数</h2><p><strong>[标定时间]</strong><br>10 Nov 2023</p><p><strong>[image]</strong><br>width x height &#x3D;  640 x 480</p><p><strong>[narrow_stereo]</strong><br><strong>camera matrix</strong><br>542.266049 0.000000 335.743717<br>0.000000 541.768934 234.053530<br>0.000000 0.000000 1.000000</p><p><strong>distortion</strong><br>0.055829 -0.287435 0.004553 0.008338 0.000000</p><p><strong>rectification</strong><br>1.000000 0.000000 0.000000<br>0.000000 1.000000 0.000000<br>0.000000 0.000000 1.000000</p><p><strong>projection</strong><br>529.754089 0.000000 342.777112 0.000000<br>0.000000 541.307739 234.928954 0.000000<br>0.000000 0.000000 1.000000 0.000000</p>]]></content>
      
      
      <categories>
          
          <category> Car-Info </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Car-Info </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这是第一篇本站的第一篇文章</title>
      <link href="/2023/11/04/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/"/>
      <url>/2023/11/04/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="创站帖"><a href="#创站帖" class="headerlink" title="创站帖"></a>创站帖</h1><p>本站建立于<strong>2023-11-4</strong><br>由<strong>Damon</strong>创建，作为私人博客</p><p>本站基于<strong>Hexo</strong>架构<br>使用<strong>butterfly</strong>主题<br>基于<strong>github</strong>、<strong>vercel</strong>实现网页代码托管<br>使用<strong>阿里云</strong>实现国内访问的映射</p><p>主站：wxtcon.github.io<br>vercel映射：wxtcon.vercel.app<br>阿里云映射：<a href="http://www.damonai.cn/">www.damonai.cn</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
