<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>WeeklyReport-004-10Dec2023.md</title>
      <link href="/2023/12/10/WeeklyReport-004-10Dec2023-md/"/>
      <url>/2023/12/10/WeeklyReport-004-10Dec2023-md/</url>
      
        <content type="html"><![CDATA[<h1 id="工作周报"><a href="#工作周报" class="headerlink" title=" 工作周报 "></a><p style="text-align: center;"> 工作周报 </p></h1><p style="text-align: center;"> <b>时间: </b> 2023-12-04 ~ 2023-12-10 </p><br><h2 id="一、本周做了什么"><a href="#一、本周做了什么" class="headerlink" title="一、本周做了什么"></a>一、本周做了什么</h2><h3 id="1-训练GNSS中断场景下的LSTM网络"><a href="#1-训练GNSS中断场景下的LSTM网络" class="headerlink" title="1. 训练GNSS中断场景下的LSTM网络"></a>1. 训练GNSS中断场景下的LSTM网络</h3><p>使用sport_field数据集前5分钟数据进行训练和推理，其中前四分钟用于训练，后一分钟用于模型推理。</p><p><strong>第一种：端到端方案</strong><br>训练时：<br>模型输入为：经过IMU预积分后的位姿（P、V、Q），GVINS融合后的位姿（GNSS有效）<br>想让模型可以学习到这样的函数关系：GNSS有效下的融合位姿 &#x3D; f(IMU预积分后的位姿)<br>评价：这个思路是端到端的来看LSTM的效果，有点极端。GNSS失效时，仅仅依靠模型推理融合后的位姿。</p><p><strong>第二种：退化松耦合方案</strong><br>模型输入为：经过IMU预积分后的位姿（P、V、Q），GNSS有效情况时输出的经纬高（LLA）<br>想让模型可以学习到这样的函数关系：LLA &#x3D; f(IMU预积分后的位姿)</p><h3 id="2-构建LSTM效果评估方法"><a href="#2-构建LSTM效果评估方法" class="headerlink" title="2. 构建LSTM效果评估方法"></a>2. 构建LSTM效果评估方法</h3><p>评估采用的工具是evo，采用RTK在fix解下的LLA数据作为真值，仅评估三轴位置精度，，所有的结果均转换到东北天（ENU）坐标系下进行评价<br>评估指标采用业界常用的APE和RPE指标，关注各个指标的最大值、最小值、平均值和RMSE四个评价量<br>目前评估流程已经搭建完毕</p><p>轨迹界面：<br><img src="image-1.png" alt="Alt text"><br>东-北-天精度展示界面：<br><img src="image-2.png" alt="Alt text"><br>APE评估：<br><img src="image-3.png" alt="Alt text"><br>RPE评估：<br><img src="image-4.png" alt="Alt text"></p><h3 id="3-解决一些工程问题"><a href="#3-解决一些工程问题" class="headerlink" title="3. 解决一些工程问题"></a>3. 解决一些工程问题</h3><p><strong>3.1 ECEF-&gt; ENU, LLA-&gt;ENU转化函数的编写</strong><br>由于RTK真值数据的结果只有一个LLA数据，GVINS输出的是ECEF坐标系下数据，但是最终效果评估需要在ENU坐标系下进行，故需要编写ECEF-&gt; ENU, LLA-&gt;ENU函数。<br><strong>3.2 GNSS原始数据的保存</strong><br>因为下周想着让模型能学习GNSS原始数据与IMU之间的关系，所以需要想法子将GNSS原始数据保存下来。与IMU这种定长的数据不同，GNSS在每个时刻锁定的卫星数量是不确定的，所以不方便使用csv文件保存，故而采用json文件保存。我这种花了少量时间调研了C++常用的几种json文件读写工具，最终采用rapidjson实现卫星原始数据的保存，并学习了Rapidjson的用法，编写了相应的代码，目前已经完成了这部分功能的编写和测试，均工作正常。<br><strong>3.3 解决闰秒问题</strong><br>ROS端使用的是UTC时间，RTK使用的是GPS时间，GPS时间和UTC时间会存在一个闰秒问题，造成两者时间不同步，给评估工作带来麻烦</p><p>两者的转换关系是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UTC_time = GPS_time - err_sec</span><br></pre></td></tr></table></figure><p>其中，err_sec是一个时间差，主要由闰秒问题产生，如果各个时间完全精确的话，err_sec应该是18.0秒整，但是由于接收机的原因，两者肯定不会这么准，所以需要在线计算，现在是上电时锁定GNSS数据后就在线计算一个err_sec，目前这个值大概是17.9几左右，需要在评估的时候手动对RTK时间进行修正<br><img src="image.png" alt="Alt text"></p><p>此外，由于闰秒的存在，DT长度总会比GT短18秒左右。评估指令中需要注意</p><h2 id="二、下周怎么安排"><a href="#二、下周怎么安排" class="headerlink" title="二、下周怎么安排"></a>二、下周怎么安排</h2><ol><li>评估网络训练效果</li><li>结合文献、评估效果对网络参数、网络结构、网络输入输出进行优化</li><li>做完GNSS原始量测生成的实验，并评估效果</li></ol>]]></content>
      
      
      <categories>
          
          <category> NaN </category>
          
          <category> NaN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>WeeklyReport-003-03Dec2023</title>
      <link href="/2023/12/03/WeeklyReport-003-03Dec2023/"/>
      <url>/2023/12/03/WeeklyReport-003-03Dec2023/</url>
      
        <content type="html"><![CDATA[<h2 id="一、本周做了什么"><a href="#一、本周做了什么" class="headerlink" title="一、本周做了什么"></a>一、本周做了什么</h2><h3 id="1-GVINS-代码功能增加"><a href="#1-GVINS-代码功能增加" class="headerlink" title="1. GVINS 代码功能增加"></a>1. GVINS 代码功能增加</h3><p>1.1 捋清GVINS代码逻辑<br>进一步研究了GVINS的代码逻辑，理清了其中的关键变量、关键函数，增加了详细的注释<br>在我的github主页可以看到<br><a href="mailto:&#x67;&#105;&#116;&#x40;&#x67;&#105;&#116;&#x68;&#117;&#98;&#46;&#99;&#x6f;&#x6d;">&#x67;&#105;&#116;&#x40;&#x67;&#105;&#116;&#x68;&#117;&#98;&#46;&#99;&#x6f;&#x6d;</a>:wxtcon&#x2F;gvins_comments_by_damon.git<br>（对GVINS的所有修改都会push到github上）</p><p>1.2 增加虚拟串口输出功能<br>虚拟串口主要是为了方便代码调试，可以通过虚拟串口在另一个终端打印调试信息，并通过终端输入调试参数</p><p>1.3 增加地图轨迹显示功能<br>地图轨迹显示主要是为了写论文的时候将真实轨迹和算法的推理轨迹放在叠加在google地图中（我看别人的论文都是这么做的）<br>我使用python写了一个脚本，实现了输入LLA在地图上显示轨迹的功能<br>代码在我的github上，路径为：GVINS&#x2F;script&#x2F;map_track_visual.py<br>效果如图所示：<br><img src="f9f4809365b12a726a137a320660f250.JPG" alt="Alt text"></p><h3 id="2-数据集生成"><a href="#2-数据集生成" class="headerlink" title="2. 数据集生成"></a>2. 数据集生成</h3><p>在GVINS中增加了保存数据的功能，主要包含IMU测量、image测量、GNSS测量，并写入GVINS文件</p><h3 id="3-NN方案设计"><a href="#3-NN方案设计" class="headerlink" title="3. NN方案设计"></a>3. NN方案设计</h3><p>设计了三种GNSS中断条件下的LSTM网络，具体的网络输入输出设计周六已经同师兄已经讨论过，目前正在加紧训练</p><h3 id="4-学习-充电"><a href="#4-学习-充电" class="headerlink" title="4. 学习&#x2F;充电"></a>4. 学习&#x2F;充电</h3><p>总结SLAM中各种优化的区别（附录）</p><h2 id="二、下周怎么安排"><a href="#二、下周怎么安排" class="headerlink" title="二、下周怎么安排"></a>二、下周怎么安排</h2><p>下周主要是对设计的几种网络进行训练，主要包含：</p><ol><li><p>复现 delta_P_GNSS &#x3D; f(IMU, T)网络<br>这部分已经在做了，但是目前还没搞出来评估网络输出delta_P的效果的方法，计划先把这个网络复现了<br>与原论文呢不同的是，我的这个网络考虑了GNSS失效时间T，原论文没考虑</p></li><li><p>复现 raw_measurement_GNSS &#x3D; f(IMU, visual, T)<br>这里的期望输出是卫星的原始量测</p></li><li><p>复现 LLA_GNSS &#x3D; f(IMU, visual, T)<br>这里的输出的LLA直接是经纬高</p></li><li><p>在训练的时候，设计一种GNSS输出效果的评估指标&#x2F;方式，用于评估网络的训练效果</p></li></ol><h2 id="附录：谈一谈对SLAM中何种“优化的理解”"><a href="#附录：谈一谈对SLAM中何种“优化的理解”" class="headerlink" title="附录：谈一谈对SLAM中何种“优化的理解”"></a>附录：谈一谈对SLAM中何种“优化的理解”</h2><h3 id="对优化的理解"><a href="#对优化的理解" class="headerlink" title="对优化的理解"></a>对优化的理解</h3><h4 id="非线性优化"><a href="#非线性优化" class="headerlink" title="非线性优化"></a>非线性优化</h4><p>从直观上来说，优化就是沿着梯度下降的方向去找到使误差(代价函数)最小化的优化变量。<br>非线性优化，自然就是误差函数和需要优化的变量之间不是简单的线性关系，而是非线性的。<br>常用的库有ceres。</p><h4 id="图优化"><a href="#图优化" class="headerlink" title="图优化"></a>图优化</h4><p>图优化的本质还是优化，只不过是用图的方式去表达。把优化变量当做顶点，约束当做边，也是用梯度下降的方法去使误差最小化。<br>图优化的好处：因为slam的特性，图是稀疏的，因此可以用图理论去进行边缘化，消元，加速计算。<br>常用的库有g2o。</p><h4 id="因子图优化"><a href="#因子图优化" class="headerlink" title="因子图优化"></a>因子图优化</h4><p>和图优化不同的是，它的重点在因子图。<br>它的优化落脚点不再是最小二乘理论了，而是最大后验概率理论了。</p><p>因子图优化可以在问题的拓扑结构上进行一些顶层抽象和简化，而不是直接硬解，相当于多了一步简化过程。<br>如果只有路标和位姿之间的因子，和BA优化完全一样。不过因子图是个大筐，什么约束都能加，IMU，轮速计，GPS。<br>边缘化掉节点之后还能保留稍微复杂一些的分布，信息损失较少。还有isam，可以在顶层结构上分析出新的测量值进来之后哪些节点需要优化，节省资源。</p><p>常用库有gtsam</p><p><strong>需要注意的是：</strong></p><ol><li>如果先验概率分布和观测噪声都是高斯分布，同时系统的动力学是线性的，那么MAP估计和最小二乘估计是等价的。<br>这是因为在这种情况下，MAP估计就是在最小二乘目标函数上加上一个正则化项，该正则化项对应于负对数先验概率。</li><li>当系统的观测模型是线性的时，MAP估计也等价于最小二乘估计。在这种情况下，MAP估计的最优解与最小二乘估计的解相同。</li><li>在非高斯、非线性的情况下，MAP估计可以通过使用数值优化方法来解决。这可能涉及到使用梯度下降、牛顿法、粒子滤波等技术。在这种情况下，MAP估计和最小二乘估计可能会产生不同的结果。</li><li>等价性的条件是有限制的，MAP估计更一般地适用于各种概率分布和模型。</li></ol><h4 id="位姿图优化"><a href="#位姿图优化" class="headerlink" title="位姿图优化"></a>位姿图优化</h4><p>位姿图优化，假设路标点是确定的，优化变量只有位姿，路标点成了约束。<br>位姿图是简化的BA<br>位姿图优化就是把各个位姿构成误差方程，优化各个位姿，不涉及路标点<br>位姿图需要考虑回环检测，最终矩阵可能不是稀疏矩阵。但好在不需要实时运行，cpu有空闲就算一些，有空闲就算一些，所以矩阵不稀疏也不是什么大问题。</p><h4 id="BA优化"><a href="#BA优化" class="headerlink" title="BA优化"></a>BA优化</h4><p>位姿图优化和BA优化都是slam中的具体问题<br>带有相机位姿和空间点的图优化称为BA<br>BA是SLAM和SFM定义出来的一个优化问题，以重投影误差为误差函数，以每个时刻位姿和所有路标点为优化变量<br>BA优化中，路标点和位姿都是不确定的，都是优化变量<br>BA优化，路标点数量远大于位姿数量，是一个箭头型矩阵，可以用舒尔补的方法转换成三角阵快速求解。在前端中，需要实时求解。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>本质都是非线性优化<br>因子图优化或者图优化指大规模稀疏的非线性优化，采用遍历图的方式去解非线性优化问题中的非线性方程组。<br>位姿图优化和ba优化是针对特定问题的图优化。<br>如果不去深究优化时选用什么样的矩阵分解方法的话，可以理解成都是一回事。<br>说白了，最后就是加权平均，没什么高深的。科技以改名为主。</p>]]></content>
      
      
      <categories>
          
          <category> NaN </category>
          
          <category> NaN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>周报-002-26Nov2023</title>
      <link href="/2023/11/26/WeeklyReport-002-26Nov2023/"/>
      <url>/2023/11/26/WeeklyReport-002-26Nov2023/</url>
      
        <content type="html"><![CDATA[<h1 id="工作周报"><a href="#工作周报" class="headerlink" title=" 工作周报 "></a><p style="text-align: center;"> 工作周报 </p></h1><p style="text-align: center;"> <b>时间: </b> 2023-11-20 ~ 2023-11-26 </p><br><h2 id="一、本周做了什么"><a href="#一、本周做了什么" class="headerlink" title="一、本周做了什么"></a>一、本周做了什么</h2><h3 id="1-pytorch扫盲"><a href="#1-pytorch扫盲" class="headerlink" title="1. pytorch扫盲"></a>1. pytorch扫盲</h3><p>学习pytorch的常用模块，并以CIFAR10编程实现一个图片分类网络以熟悉使用费pytorch进行NN搭建&amp;训练的流程<br>学习笔记：<a href="https://www.damonai.cn/2023/11/25/learn-pytorch/">https://www.damonai.cn/2023/11/25/learn-pytorch/</a><br>学习代码：github.com&#x2F;wxtcon&#x2F;learn_pytorch</p><h3 id="2-GVINS-dataset数据提取"><a href="#2-GVINS-dataset数据提取" class="headerlink" title="2. GVINS-dataset数据提取"></a>2. GVINS-dataset数据提取</h3><p>为了实现离线训练NN，需要将ros bag转为csv格式数据，写了一个脚本，实现rosbag按topic提取数据并写入csv中</p><h3 id="3-DL扫盲"><a href="#3-DL扫盲" class="headerlink" title="3. DL扫盲"></a>3. DL扫盲</h3><p>参考B站资料对Loss Fun、正则化、CNN、RNN、LSTM、GRU等知识点进行了扫盲</p><h2 id="二、下周怎么安排"><a href="#二、下周怎么安排" class="headerlink" title="二、下周怎么安排"></a>二、下周怎么安排</h2><ol><li>写一个提取GVINS误差的代码<br>原始的GVINS-dataset中只有星历、伪距这些数据，不能进行网络的训练，无法直接在GNSS拒止状态下输出GVINS误差，需要先跑一遍GVINS代码，把各个传感器的误差项提取出来</li><li>用GNSS误差数据对LSTM—NN进行离线训练，并评估效果</li></ol>]]></content>
      
      
      <categories>
          
          <category> NaN </category>
          
          <category> NaN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch入门笔记</title>
      <link href="/2023/11/25/learn-pytorch/"/>
      <url>/2023/11/25/learn-pytorch/</url>
      
        <content type="html"><![CDATA[<h1 id="Pytorch入门笔记"><a href="#Pytorch入门笔记" class="headerlink" title="Pytorch入门笔记"></a>Pytorch入门笔记</h1><p><strong>参考资源：bilibili - 我是土堆</strong><br>url : <a href="https://www.bilibili.com/video/BV1hE411t7RN/?spm_id_from=333.999.0.0&vd_source=9761a01d2f08a7425632b3ad97cccf18">https://www.bilibili.com/video/BV1hE411t7RN/?spm_id_from=333.999.0.0&amp;vd_source=9761a01d2f08a7425632b3ad97cccf18</a></p><h1 id="一、-数据加载与预处理"><a href="#一、-数据加载与预处理" class="headerlink" title="一、 数据加载与预处理"></a>一、 数据加载与预处理</h1><h2 id="1-pytorch加载数据"><a href="#1-pytorch加载数据" class="headerlink" title="1. pytorch加载数据"></a>1. pytorch加载数据</h2><p><strong>Dataset的使用</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.data import Dataset</span><br><span class="line">from PIL import Image</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">class MyData(Dataset):</span><br><span class="line"></span><br><span class="line">    def __init__(self, root_dir, label_dir):</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.label_dir = label_dir</span><br><span class="line">        self.path = os.path.join(self.root_dir, self.label_dir)</span><br><span class="line">        self.img_path = os.listdir(self.path)</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        img_name = self.img_path[index]</span><br><span class="line">        img_item_path = os.path.join(self.root_dir, self.label_dir, img_name)</span><br><span class="line">        img = Image.open(img_item_path)</span><br><span class="line">        label = self.label_dir</span><br><span class="line"></span><br><span class="line">        return img, label</span><br><span class="line">    </span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.img_path)</span><br><span class="line">    </span><br><span class="line">root_dir = &#x27;dataset/Type1/train&#x27;</span><br><span class="line">ants_label_dir = &#x27;ants&#x27;</span><br><span class="line">bees_label_dir = &#x27;bees&#x27;</span><br><span class="line"></span><br><span class="line">ants_dataset = MyData(root_dir, ants_label_dir)</span><br><span class="line">bees_dataset = MyData(root_dir, bees_label_dir)</span><br><span class="line"></span><br><span class="line">train_dataset = ants_dataset + bees_dataset</span><br><span class="line"></span><br><span class="line">img, label = train_dataset[0]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="2-TensorBoard的使用"><a href="#2-TensorBoard的使用" class="headerlink" title="2. TensorBoard的使用"></a>2. TensorBoard的使用</h2><p>用于训练模型的时候显示图像（LOSS曲线）等</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line">import numpy as np</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(&quot;logs&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">img_path = &#x27;dataset/Type1/train/ants/6240329_72c01e663e.jpg&#x27;</span><br><span class="line">img = Image.open(img_path)</span><br><span class="line">img_array = np.array(img)</span><br><span class="line"></span><br><span class="line">writer.add_image(&#x27;test&#x27;, img_array, 1, dataformats=&#x27;HWC&#x27;) # 显示图像</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># for i in range(100):</span><br><span class="line">#     writer.add_scalar(&#x27;y = x&#x27;, 2*i, i) # 显示标量数据</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># writer.add_scalar()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>在终端输入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=logs</span><br></pre></td></tr></table></figure><p>在浏览器中即可看到显示的图像</p><h2 id="3-Transforms的使用"><a href="#3-Transforms的使用" class="headerlink" title="3. Transforms的使用"></a>3. Transforms的使用</h2><p>Transforms主要是对图像进行一些变换</p><p><strong>a. Transforms 应该如何使用</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from PIL import Image</span><br><span class="line">from torchvision import transforms</span><br><span class="line"></span><br><span class="line">img_path = &quot;dataset/Type1/train/ants/6743948_2b8c096dda.jpg&quot;</span><br><span class="line"></span><br><span class="line">img = Image.open(img_path)</span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor() # 对象实例化</span><br><span class="line">tensor_img = tensor_trans(img)</span><br><span class="line"></span><br><span class="line">print(tensor_img)</span><br></pre></td></tr></table></figure><p><strong>b. 为什么需要使用tensor的数据类型</strong></p><p>tensor可以简单粗暴的理解为将图片等传统的数据封装为神经网络所需的数据类型</p><p><strong>c. 常见的transforms</strong></p><ol><li><p>Compose<br> compose()中的参数需要是一个列表，且列表中的数据需要时transforms类型，即：<br> compose([tansforms参数1， tansforms参数2])</p><p> 在我理解compose是将多个图像操作合起来了，例如将图像先裁剪再转换为tensor格式<br> 【tansforms参数1】的输出需要时【tansforms参数2】的输入，注意格式需要满足衔接需求</p></li><li><p>ToTensor<br> 将PIL或者opencv的数据格式转换为tensor的格式</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor_trans = transforms.ToTensor() </span><br><span class="line">tensor_img = tensor_trans(img)</span><br></pre></td></tr></table></figure></li><li><p>Normalize<br> 归一化</p> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trans_norm = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) # mean std</span><br><span class="line">img_norm = trans_norm(tensor_img)</span><br></pre></td></tr></table></figure></li><li><p>Resize<br>trans_resize &#x3D; transforms.Resize((128, 128))<br>or<br>trans_resize &#x3D; transforms.Resize((128))<br>img_resize &#x3D; trans_resize(img)</p></li><li><p>RandomCrop<br>用法同Resize</p></li></ol><h2 id="4-torchvision中的Dataset使用"><a href="#4-torchvision中的Dataset使用" class="headerlink" title="4. torchvision中的Dataset使用"></a>4. torchvision中的Dataset使用</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset_transform = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=&quot;./my_dataset&quot;,</span><br><span class="line">                                         train=True,</span><br><span class="line">                                         download=True,</span><br><span class="line">                                         transform=dataset_transform)</span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=&quot;./my_dataset&quot;,</span><br><span class="line">                                        train=False,</span><br><span class="line">                                        download=True,</span><br><span class="line">                                        transform=dataset_transform)</span><br><span class="line"></span><br><span class="line"># dataset_transforms = torchvision.transforms.Compose([])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># img.show()</span><br><span class="line">writer = SummaryWriter(&#x27;pic10&#x27;)</span><br><span class="line"></span><br><span class="line">print(test_set[0])</span><br><span class="line"></span><br><span class="line">for i in range(10):</span><br><span class="line">    img, target = test_set[i]</span><br><span class="line">    writer.add_image(&#x27;pic&#x27;, img, i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h2 id="5-DataLoader的使用"><a href="#5-DataLoader的使用" class="headerlink" title="5. DataLoader的使用"></a>5. DataLoader的使用</h2><p>DataLoader参数说明：<br>– dataset：打包好的tensor格式的数据<br>– batch_size： 一轮抓数据的个数（每一轮抓图片的张数）<br>– shuffle： 每个epoch结束后是否打乱所有图片（True为打乱，False为不打乱）<br>– num_workers: 多线程运行数（在windows下运行时此变量若大于1可能会出错）<br>– drop_last：最后一组数据不够一个batch_size数量时，是否丢弃，True为丢弃，False为不丢弃</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset_transform = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(&quot;./my_dataset&quot;, train=False, transform=dataset_transform, download=True)</span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=128, shuffle=True, num_workers=0, drop_last=False)</span><br><span class="line"></span><br><span class="line">img, target = test_data[0]</span><br><span class="line">print(img.shape)</span><br><span class="line">print(target)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(&#x27;dataloader&#x27;)</span><br><span class="line"></span><br><span class="line">step = 0</span><br><span class="line">for data in test_loader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    # print(imgs.shape)</span><br><span class="line">    # print(targets)</span><br><span class="line">    writer.add_images(&quot;dataloader&quot;, imgs, step)</span><br><span class="line">    step = step + 1</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="二、神经网络"><a href="#二、神经网络" class="headerlink" title="二、神经网络"></a>二、神经网络</h1><h2 id="6-神经网络基本骨架nn-Module的使用"><a href="#6-神经网络基本骨架nn-Module的使用" class="headerlink" title="6. 神经网络基本骨架nn.Module的使用"></a>6. 神经网络基本骨架nn.Module的使用</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">class Damon(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__() # 调用父类的方法</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = input + 1</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">damon = Damon()</span><br><span class="line">x = torch.tensor(1.0)</span><br><span class="line">output = damon(x)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure><h2 id="7-卷积操作"><a href="#7-卷积操作" class="headerlink" title="7. 卷积操作"></a>7. 卷积操作</h2><ol><li>torch.nn是对torch.nn.function的封装，一般会torch.nn即可</li><li>conv2d()函数操作<br> stride：步进，即每次卷积核移动的距离<br> padding：图片边缘掩膜填充</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">input = torch.tensor([[1, 2, 0, 3, 1],</span><br><span class="line">                      [0, 1, 2, 3, 1],</span><br><span class="line">                      [1, 2, 1, 0, 0],</span><br><span class="line">                      [5, 2, 3, 1, 1],</span><br><span class="line">                      [2, 1, 0, 1, 1]])</span><br><span class="line"></span><br><span class="line">kernel = torch.tensor([[1, 2, 1],</span><br><span class="line">                       [0, 1, 0],</span><br><span class="line">                       [2, 1, 0]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">input = torch.reshape(input, (1, 1, input.shape[0], input.shape[1]))</span><br><span class="line">kernel = torch.reshape(kernel, (1, 1, kernel.shape[0], kernel.shape[1]))</span><br><span class="line"></span><br><span class="line">output = F.conv2d(input, kernel, stride=1, padding=1)</span><br><span class="line"></span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure><h2 id="8-卷积层"><a href="#8-卷积层" class="headerlink" title="8. 卷积层"></a>8. 卷积层</h2><p>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride&#x3D;1, padding&#x3D;0, dilation&#x3D;1, groups&#x3D;1, bias&#x3D;True, padding_mode&#x3D;’zeros’, device&#x3D;None, dtype&#x3D;None)</p><p>– in_channels： 输入图像的通道数，一般彩色图像都是3<br>– out_channels： 输出通道数<br>– kernel_size ： 卷积核大小，例如3（即3<em>3的卷积核） （1，2）（即1</em>2的卷积核，不规则卷积核），定义的时候只需要设定卷积核大小，卷积核的值会自动采样得到<br>– stride ： 卷积核步进<br>– padding：边缘填充<br>– padding_mode&#x3D;’zeros’： 以0填充<br>– bias：偏置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import Conv2d</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(&quot;./my_dataset&quot;,</span><br><span class="line">                                       train=False,</span><br><span class="line">                                       transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=True)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=64)</span><br><span class="line"></span><br><span class="line">class Damon(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Damon, self).__init__()</span><br><span class="line">        self.conv1 = Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=0)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">damon = Damon()</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(&#x27;./logs&#x27;)</span><br><span class="line"></span><br><span class="line">step = 0</span><br><span class="line">for data in dataloader:</span><br><span class="line">    step = step + 1</span><br><span class="line">    imgs, targets = data</span><br><span class="line"></span><br><span class="line">    output = damon(imgs)</span><br><span class="line">    # print(output.shape)</span><br><span class="line">    writer.add_images(&#x27;input&#x27;, imgs, step)</span><br><span class="line"></span><br><span class="line">    # output = torch.reshape(output, (-1, 3, 30, 30))</span><br><span class="line">    # output = output.view(-1, 3, 30, 30)</span><br><span class="line">    writer.add_images(&#x27;output&#x27;, output, step)</span><br></pre></td></tr></table></figure><h2 id="9-池化层"><a href="#9-池化层" class="headerlink" title="9. 池化层"></a>9. 池化层</h2><p>最大池化又称下采样<br>最大池化的作用是：保持输入数据的特征，同时减小数据量，网络参数会减少，训练速度会更快<br>所以在很多网络中，大家卷积完之后都会来一次最大池化，然后再来一次非线性激活</p><p>torch.nn.MaxPool2d(kernel_size, stride&#x3D;None, padding&#x3D;0, dilation&#x3D;1, return_indices&#x3D;False, ceil_mode&#x3D;False)<br>参数：<br>– kernel_size (Union[int, Tuple[int, int]]) – the size of the window to take a max over</p><p>– stride (Union[int, Tuple[int, int]]) – the stride of the window. Default value is <strong>kernel_size</strong> （区别卷积层的default是1）</p><p>– padding (Union[int, Tuple[int, int]]) – Implicit negative infinity padding to be added on both sides</p><p>– dilation (Union[int, Tuple[int, int]]) – a parameter that controls the stride of elements in the window （设置空洞卷积）</p><p>– return_indices (bool) – if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later</p><p>– ceil_mode (bool) – when True, will use ceil instead of floor to compute the output shape （default &#x3D; false）<br>ceil和floor的区别看下图</p><p><img src="image.png" alt="Alt text"><br>ceil和floor的具体区别直观展示：<br><img src="image-1.png" alt="Alt text"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import MaxPool2d</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(root=&#x27;./my_dataset&#x27;,</span><br><span class="line">                                       train=False,</span><br><span class="line">                                       transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=True)</span><br><span class="line">dataload = DataLoader(dataset, batch_size=64)</span><br><span class="line"></span><br><span class="line"># input = torch.tensor([[1, 2, 0, 3, 1],</span><br><span class="line">#               [0, 1, 2, 3, 1],</span><br><span class="line">#               [1, 2, 1, 0, 0],</span><br><span class="line">#               [5, 2, 3, 1, 1],</span><br><span class="line">#               [2, 1, 0, 1, 1]], dtype=torch.float32)</span><br><span class="line"># input = torch.reshape(input, (-1, 1, 5, 5))</span><br><span class="line"></span><br><span class="line">class Damon(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Damon, self).__init__()</span><br><span class="line">        self.maxpool1 = MaxPool2d(kernel_size=3, ceil_mode=False)</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = self.maxpool1(input)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">damon = Damon();</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(&#x27;logs&#x27;)</span><br><span class="line">step = 0</span><br><span class="line">for data in dataload:</span><br><span class="line">    step = step + 1</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = damon(imgs)</span><br><span class="line">    writer.add_images(&#x27;maxpool&#x27;, output, step)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="10-padding层"><a href="#10-padding层" class="headerlink" title="10. padding层"></a>10. padding层</h2><p>对输入图像周围进行填充<br>一般用不到，因为一般在conv中我们会通过padding参数进行填充</p><p>eg：nn.ZeroPad2d</p><ul><li>Pads the input tensor boundaries with zero.</li></ul><h2 id="11-非线性激活"><a href="#11-非线性激活" class="headerlink" title="11. 非线性激活"></a>11. 非线性激活</h2><p>非线性变换的主要目的就是为了给网络中引入一些非线性特征<br>因为非线性特征越多的话，模型泛化能力越好</p><p>eg:<br><strong>–ReLu:</strong><br>小于0时进行阶段，大于0时原样输出<br><img src="image-2.png" alt="Alt text"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import ReLU</span><br><span class="line"></span><br><span class="line">input = torch.tensor([[1, -0.5],</span><br><span class="line">             [-0.2, 3]])</span><br><span class="line"></span><br><span class="line">class Damon(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Damon, self).__init__()</span><br><span class="line">        self.relu1 = ReLU()</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = self.relu1(input)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">damon = Damon()</span><br><span class="line">output = damon(input)</span><br><span class="line"></span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure><p><strong>– sigmoid</strong><br><img src="image-3.png" alt="Alt text"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import Sigmoid</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(root=&#x27;./my_dataset&#x27;,</span><br><span class="line">                                       train=False,</span><br><span class="line">                                       transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=True)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=64)</span><br><span class="line"></span><br><span class="line">class Damon(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Damon, self).__init__()</span><br><span class="line">        self.sigmoid1 = Sigmoid()</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = self.sigmoid1(input)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">damon = Damon()</span><br><span class="line">writer = SummaryWriter(&#x27;./logs&#x27;)</span><br><span class="line">step = 0</span><br><span class="line">for data in dataloader:</span><br><span class="line">    step = step + 1</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = damon(imgs)</span><br><span class="line">    writer.add_images(&#x27;sigmoid&#x27;, output, step)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h2 id="12-线性层及其它层"><a href="#12-线性层及其它层" class="headerlink" title="12. 线性层及其它层"></a>12. 线性层及其它层</h2><ol><li>正则化层<br>采用正则化层可以加快网络的训练速度<br>用的比较少</li><li>Recurrent Layers<br>RNN &#x2F;LSTM etc<br>属于特定的网络结构</li><li>Transformer Layer</li><li>线性层<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import Linear</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(&quot;./my_dataset&quot;,</span><br><span class="line">                                       train=False,</span><br><span class="line">                                       transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=True)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=64)</span><br><span class="line"></span><br><span class="line">class Damon(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Damon, self).__init__()</span><br><span class="line">        self.linear1 = Linear(196608,10)</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = self.linear1(input)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">t = Damon()</span><br><span class="line">for data in dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    imgs = torch.flatten(imgs)</span><br><span class="line">    output = t(imgs)</span><br><span class="line">    print(output.shape)</span><br></pre></td></tr></table></figure></li><li>Dropout Layer<br>为了防止模型过拟合</li><li>Flatten 展平<br>from torch.nn import Flatten<br>self.flatten &#x3D; Flatten()</li></ol><h2 id="13-NN-搭建实战和Sequential的使用"><a href="#13-NN-搭建实战和Sequential的使用" class="headerlink" title="13. NN 搭建实战和Sequential的使用"></a>13. NN 搭建实战和Sequential的使用</h2><p><strong>a. Sequential的使用</strong><br>Sequential类似于compose,是将多个操作（层）合起来的一个工具</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">          nn.Conv2d(1,20,5),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(20,64,5),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">          (&#x27;conv1&#x27;, nn.Conv2d(1,20,5)),</span><br><span class="line">          (&#x27;relu1&#x27;, nn.ReLU()),</span><br><span class="line">          (&#x27;conv2&#x27;, nn.Conv2d(20,64,5)),</span><br><span class="line">          (&#x27;relu2&#x27;, nn.ReLU())</span><br><span class="line">        ]))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>b. CIFAR10 Network_model</strong><br><img src="image-4.png" alt="Alt text"></p><p>Tips：使用TensorBoard中的add_graph可以将网络展开成如下形状，便于直观检查网络的问题</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writer = SummaryWriter(&#x27;./logs&#x27;)</span><br><span class="line">writer.add_graph(damon, input)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p><img src="image-5.png" alt="Alt text"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">class Damon(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Damon, self).__init__()</span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(3, 32, 5, padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Conv2d(32, 32, 5, padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Conv2d(32, 64, 5, padding=2),</span><br><span class="line">            MaxPool2d(2),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(1024, 64),</span><br><span class="line">            Linear(64, 10)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        output = self.model1(input)</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">damon = Damon()</span><br><span class="line"></span><br><span class="line">input = torch.ones((64, 3, 32, 32))</span><br><span class="line">output = damon(input)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(&#x27;./logs&#x27;)</span><br><span class="line">writer.add_graph(damon, input)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="三、-pytorch应用"><a href="#三、-pytorch应用" class="headerlink" title="三、 pytorch应用"></a>三、 pytorch应用</h1><h2 id="14-损失函数与反向传播"><a href="#14-损失函数与反向传播" class="headerlink" title="14. 损失函数与反向传播"></a>14. 损失函数与反向传播</h2><p>a. Loss Fun的作用<br>    1.计算实际输出和实际目标之间的差距<br>    2.为参数更新提供依据（反向传播）<br>    实际上神经网络的参数就是卷积核的参数，参数更新就是根据loss计算参数的梯度，以便进行梯度下降，达到Loss下降的目的<br>b. 常用的LOSS函数<br>    L1Loss<br>    MSELoss<br>    CrossEntropyLoss 交叉熵损失 分类中常用<br><img src="image-6.png" alt="Alt text"><br>c. 反向传播代码<br>反向传播会求出每一个参数的梯度</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for data in dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = damon(imgs)</span><br><span class="line">    print(output)</span><br><span class="line">    print(&#x27;target:&#x27;, targets)</span><br><span class="line">    loss = celoss(output, targets)</span><br><span class="line">    loss.backward()</span><br><span class="line">    print(&#x27;loss:&#x27;, loss)</span><br><span class="line">    exit()</span><br></pre></td></tr></table></figure><h2 id="15-优化器"><a href="#15-优化器" class="headerlink" title="15. 优化器"></a>15. 优化器</h2><p><strong>a. 优化器的使用套路</strong></p><p>定义一个优化器，设置模型参数和学习率等参数<br><code>optim = torch.optim.SGD(damon.parameters(), lr=0.01)</code><br>↓<br>在每个batch_size中,先清零梯度，因为上一次的梯度对于本次的训练是没用的<br><code>optim.zero_grad()</code><br>↓<br>loss反向传播<br><code>loss.backward()</code><br>↓<br>执行优化,即：模型参数调优<br><code>optim.step()</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">damon = Damon() # instantiation</span><br><span class="line">celoss = nn.CrossEntropyLoss() # Loss definition</span><br><span class="line">optim = torch.optim.SGD(damon.parameters(), lr=0.01)</span><br><span class="line"></span><br><span class="line">for epoch in range(20):</span><br><span class="line">    run_loss = 0.0</span><br><span class="line">    for data in dataloader:</span><br><span class="line">        optim.zero_grad()</span><br><span class="line"></span><br><span class="line">        imgs, targets = data</span><br><span class="line">        output = damon(imgs)</span><br><span class="line"></span><br><span class="line">        loss = celoss(output, targets)</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        optim.step()</span><br><span class="line">        run_loss = run_loss + loss</span><br><span class="line">    print(run_loss)</span><br></pre></td></tr></table></figure><p><strong>b. 学习率</strong></p><p>学习速率太大，模型训练起来就很不稳定<br>学习速率太小，模型训练会很慢</p><p>一般先用大的学习速率，再用小的学习速率</p><h2 id="16-现有模型的加载使用与修改"><a href="#16-现有模型的加载使用与修改" class="headerlink" title="16. 现有模型的加载使用与修改"></a>16. 现有模型的加载使用与修改</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vgg16_raw = torchvision.models.vgg16() # 加载VGG16</span><br><span class="line"></span><br><span class="line">vgg16_raw.add_module(&#x27;my_linear&#x27;, nn.Linear(1000, 10)) # 在vgg16后面加线性层</span><br><span class="line">vgg16_raw.classifier.add_module(&quot;my_linear2&quot;, nn.Linear(100, 100)) # 在vgg16的classifier后面加线性层</span><br><span class="line">vgg16_raw.classifier[6] = nn.Linear(4096, 10) # 修改vgg16的classifier的6号网络层为Linear(4096, 10)</span><br></pre></td></tr></table></figure><h2 id="17-网络模型的保存与读取"><a href="#17-网络模型的保存与读取" class="headerlink" title="17. 网络模型的保存与读取"></a>17. 网络模型的保存与读取</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># raw method</span><br><span class="line">vgg16 = torchvision.models.vgg16()</span><br><span class="line"></span><br><span class="line">**# save method1:**</span><br><span class="line">torch.save(vgg16, &quot;vgg16_method1.pth&quot;)</span><br><span class="line"></span><br><span class="line"># load method1:</span><br><span class="line">model1 = torch.load(&quot;vgg16_method1.pth&quot;)</span><br><span class="line"># -------------------------</span><br><span class="line"></span><br><span class="line">**# save method2:** （官方推荐的方式）</span><br><span class="line">torch.save(vgg16.state_dict(), &#x27;vgg16_method2.pth&#x27;)</span><br><span class="line"></span><br><span class="line"># load method2:</span><br><span class="line">model2 = torch.load(&quot;vgg16_method2.pth&quot;) # only load model parameter, not contains the construction of the model</span><br><span class="line">vgg16.load_state_dict(torch.load(&quot;vgg16_method2.pth&quot;))</span><br></pre></td></tr></table></figure><h2 id="18-完成的模型训练套路（以CIFAR10为例）"><a href="#18-完成的模型训练套路（以CIFAR10为例）" class="headerlink" title="18. 完成的模型训练套路（以CIFAR10为例）"></a>18. 完成的模型训练套路（以CIFAR10为例）</h2><p>a. 使用正确率评估模型（分类问题中特有的评价指标）<br>意思就是：测试集中分类正确的图片占比多少？</p><p>完整的代码在github的project子文件夹下 train.py, 模型在model.py里</p><p>b. 训练模型的一些细节</p><ol><li>有的人在训练前会加damon.train()&#x2F;在验证前加damon.eval()等字样，这个不是必须的，这个代码只对某些层有效（例如Dropout、BatchNorm等），但是如果自己的网络里有这些特殊的层，就必须在训练&#x2F;测试前调用这些代码</li></ol><h2 id="19-使用GPU进行训练"><a href="#19-使用GPU进行训练" class="headerlink" title="19. 使用GPU进行训练"></a>19. 使用GPU进行训练</h2><p>a. 第一种方法<br>找到【模型】、【数据（输入+标注）】、【损失函数】，在这三个后面加一句.cuda()<br>eg:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">damon = Damon()</span><br><span class="line">damon = damon.cuda()</span><br></pre></td></tr></table></figure><p>b. 第二种方法<br>使用.todevice()方法</p><p>device &#x3D; torch.device(‘cpu’) # 使用CPU<br>         torch.device(‘cuda’) # 使用GPU<br>         torch.device(‘cuda:0’) # 有多张显卡时，使用第一张GPU<br>         torch.device(‘cuda:1’) # 有多张显卡时，使用第2张GPU</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&#x27;cpu&#x27;)</span><br><span class="line"></span><br><span class="line">damon = Damon()</span><br><span class="line">damon = damon.to(device)</span><br></pre></td></tr></table></figure><p>更常见的一种方式：<br><code>device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</code></p><p>c. 如果自己没有GPU可以去谷歌的colab蹭</p><h2 id="18-完成的模型测试套路"><a href="#18-完成的模型测试套路" class="headerlink" title="18. 完成的模型测试套路"></a>18. 完成的模型测试套路</h2><p>核心：利用已经训练好的模型吗，给他提供输入，进行模型推理</p><p>步骤：找测试数据（eg：图片）-&gt; reshape到模型接受的大小 -&gt; 输入模型推理</p><p>Attention: 如果模型实在GPU上训练的，那么使用CPU推理，在加载时需要制定map_location，即：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.load(&#x27;model.pth&#x27;, map_location=torch.device(&#x27;cpu&#x27;))</span><br></pre></td></tr></table></figure><h3 id="19-看看开源项目"><a href="#19-看看开源项目" class="headerlink" title="19. 看看开源项目"></a>19. 看看开源项目</h3><p>1. </p>]]></content>
      
      
      <categories>
          
          <category> NaN </category>
          
          <category> NaN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>周报-001-19Nov2023</title>
      <link href="/2023/11/18/WeeklyReport-001-19Nov2023/"/>
      <url>/2023/11/18/WeeklyReport-001-19Nov2023/</url>
      
        <content type="html"><![CDATA[<h1 id="工作周报"><a href="#工作周报" class="headerlink" title=" 工作周报 "></a><p style="text-align: center;"> 工作周报 </p></h1><p style="text-align: center;"> <b>时间: </b> 2023-11-13 ~ 2023-11-18 </p><br><h2 id="一、本周做了什么"><a href="#一、本周做了什么" class="headerlink" title="一、本周做了什么"></a>一、本周做了什么</h2><p>本周主要同步推进paper和硬件小车这两件事，主要有：</p><ol><li>Paper Reading：在 <strong>GNSS-Visual-Inertial-odometry(GVIO)</strong> 领域阅读文献多篇，其中精读3篇，并完成Reading Note（纸质），</li><li>Paper Writing: 完成了<strong>论文的（通用）部分</strong>，主要包含：<strong>坐标系的转换描述</strong>、<strong>因子图原理</strong>、<strong>Introduction部分的骨架</strong>等。</li><li>Code  Reading: 捋完了<strong>GVINS</strong>的框架，将其作为Code-Baseline在这上面修改。</li><li>学习LSTM，并基于LSTM完成了一个股票数据的预测，算作入门。</li><li>调校小车：摸清了 <strong>传感器数据采集→STM32数据收集和转发→PC机（ubuntu+ROS）</strong>的全流程；将计算任务转移到<strong>个人PC</strong>上运行；在线运行ORB-SLAM2纯视觉建图<strong>效果一般但不卡</strong>；将小车部分的启动步骤、参数细节整理成了文档。</li><li>制作做CAC会议海报，来重庆开会。</li></ol><h2 id="二、有什么收获-启发-问题"><a href="#二、有什么收获-启发-问题" class="headerlink" title="二、有什么收获&#x2F;启发&#x2F;问题"></a>二、有什么收获&#x2F;启发&#x2F;问题</h2><h3 id="2-1-Paper-Reading-GVIO"><a href="#2-1-Paper-Reading-GVIO" class="headerlink" title="2.1 Paper Reading - GVIO"></a>2.1 Paper Reading - GVIO</h3><p>  详情在纸质版笔记上，供论文的Related Work部分使用。</p><h3 id="2-2-Code-Reading-Writting-GVINS"><a href="#2-2-Code-Reading-Writting-GVINS" class="headerlink" title="2.2 Code  Reading &amp; Writting - GVINS"></a>2.2 Code  Reading &amp; Writting - GVINS</h3><ol><li>我详细阅读并注释了GVINS的代码，并已经push到我的github，链接：<a href="https://github.com/wxtcon/gvins_comments_by_damon">https://github.com/wxtcon/gvins_comments_by_damon</a></li><li>编写选星这部分算法代码，基于GVINS-Dataset测试，这会还没写完，预计下周写完并测试</li></ol><h3 id="2-3-调校小车"><a href="#2-3-调校小车" class="headerlink" title="2.3 调校小车"></a>2.3 调校小车</h3><p><strong>工作细节</strong></p><ol><li><p>尚未定位为什么在Jetson TX2上ORB-SLAM3非常卡（<strong>一核有难，三核围观</strong>）的原因，这个问题定位起来有点麻烦（<strong>暂缓</strong>）</p></li><li><p>鉴于问题1.难定位，我将工控机的所有任务转移到<strong>个人PC</strong>上进行，目前<strong>全流程已经打通</strong>（包括环境配置、代码编译测试、运行测试），现阶段小车长这样<br><img src="IMG_20231115_150317.jpg" alt="Alt text"><br><img src="IMG_20231115_150312.jpg" alt="Alt text"></p></li><li><p>使用<strong>camera-calibration</strong>工具完成了摄像头的标定，标定工具界面如下:<br><img src="cailb.bmp" alt="Alt text"></p></li><li><p>在PC上编译并运行ORB-SLAM2，在<strong>会议室</strong>进行纯视觉建图测试，<br><img src="1.bmp" alt="Alt text"><br>体素地图效果如下：<br>  <img src="2.bmp" alt="Alt text"><br>  可以发现下面这块地图发生了较大的偏移，这是因为这块地方都是墙壁，特征点太少了，发生了退化</p></li><li><p>将上述工作整理成了一个简单的文档备查<br>文档链接：<a href="http://www.damonai.cn/2023/11/15/Car-Info/">www.damonai.cn/2023/11/15/Car-Info/</a></p></li></ol><h3 id="2-4-LSTM学习"><a href="#2-4-LSTM学习" class="headerlink" title="2.4 LSTM学习"></a>2.4 LSTM学习</h3><p>学习LSTM原理，在服务器上跑了一个预测股票的demo，<br><img src="image.png" alt="Alt text"><br>之前没怎么搞过深度学习，正在补基础知识</p><h2 id="三、下阶段怎么安排"><a href="#三、下阶段怎么安排" class="headerlink" title="三、下阶段怎么安排"></a>三、下阶段怎么安排</h2><ol><li>计划用两天左右时间完成深度学习基础+RNN+LSTM扫盲</li><li>完成卫星选星算法代码的编写和效果测试（基于GVINS-Dataset）。</li><li>完成小车硬件升级：小车加高，先加一层50cm（200&#x2F;层，已买），后续视情况判断是否再加一层(凑1m)<br><img src="Screenshot_2023-11-15-23-25-47-030_com.taobao.tao.jpg" alt="Alt text"></li></ol><p><br><br></p>]]></content>
      
      
      <categories>
          
          <category> NaN </category>
          
          <category> NaN </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Car-Info</title>
      <link href="/2023/11/15/Car-Info/"/>
      <url>/2023/11/15/Car-Info/</url>
      
        <content type="html"><![CDATA[<h1 id="小车信息"><a href="#小车信息" class="headerlink" title=" 小车信息 "></a><p style="text-align: center;"> 小车信息 </p></h1><h2 id="一、常用配置命令"><a href="#一、常用配置命令" class="headerlink" title="一、常用配置命令"></a>一、常用配置命令</h2><ol><li><p>在工作空间下运行，安装ROS功能包全部依赖（镜像中已配置rosdep）：<br>rosdep install –from-paths src –ignore-src -r -y</p></li><li><p>指定功能包编译：<br>catkin_make -DCATKIN_WHITELIST_PACKAGES&#x3D;”功能包名”<br>解除指定功能包编译：<br>catkin_make -DCATKIN_WHITELIST_PACKAGES&#x3D;””</p></li><li><p>打开摄像头并使用rqt工具查看图像话题：<br>roslaunch turn_on_wheeltec_robot wheeltec_camera.launch<br>rqt_image_view</p></li><li><p>生成TF树pdf<br>rosrun tf view_frames</p></li><li><p>查看TF树<br>rosrun rqt_tf_tree rqt_tf_tree</p></li><li><p>nfs挂载:<br>sudo mount -t nfs 192.168.0.100:&#x2F;home&#x2F;wheeltec&#x2F;wheeltec_robot &#x2F;mnt<br>nfs解除挂载:<br>sudo umount -t nfs 192.168.0.100:&#x2F;home&#x2F;wheeltec&#x2F;wheeltec_robot &#x2F;mnt</p></li></ol><h2 id="二、常用功能命令"><a href="#二、常用功能命令" class="headerlink" title="二、常用功能命令"></a>二、常用功能命令</h2><p><strong>1.开启初始化节点</strong><br>（仅在单独开启键盘控制时需要开启 运行功能时已包括初始化节点 不需要重复开启）<br>roslaunch turn_on_wheeltec_robot turn_on_wheeltec_robot.launch<br>&#x2F;&#x2F;开启键盘控制节点<br>roslaunch wheeltec_robot_rc keyboard_teleop.launch</p><p><strong>2.ORB-SLAM2建图</strong><br>&#x2F;&#x2F;开启ORB节点<br>roslaunch turn_on_wheeltec_robot orb_slam.launch<br>&#x2F;&#x2F;开启rviz可视化工具<br>rviz -d robot_ws&#x2F;src&#x2F;orb_slam2_ros-master&#x2F;ros&#x2F;launch&#x2F;orb_slam.rviz<br>（rviz配置文件路径：orb_slam2_ros-master&#x2F;ros&#x2F;launch&#x2F;orb_slam.rviz）<br>&#x2F;&#x2F; 手动保存地图(在地图要保存路径下运行)：<br>rosrun map_server map_saver -f </p><h2 id="三、Astra-Pro相机内参数"><a href="#三、Astra-Pro相机内参数" class="headerlink" title="三、Astra Pro相机内参数"></a>三、Astra Pro相机内参数</h2><p><strong>[标定时间]</strong><br>10 Nov 2023</p><p><strong>[image]</strong><br>width x height &#x3D;  640 x 480</p><p><strong>[narrow_stereo]</strong><br><strong>camera matrix</strong><br>542.266049 0.000000 335.743717<br>0.000000 541.768934 234.053530<br>0.000000 0.000000 1.000000</p><p><strong>distortion</strong><br>0.055829 -0.287435 0.004553 0.008338 0.000000</p><p><strong>rectification</strong><br>1.000000 0.000000 0.000000<br>0.000000 1.000000 0.000000<br>0.000000 0.000000 1.000000</p><p><strong>projection</strong><br>529.754089 0.000000 342.777112 0.000000<br>0.000000 541.307739 234.928954 0.000000<br>0.000000 0.000000 1.000000 0.000000</p>]]></content>
      
      
      <categories>
          
          <category> Car-Info </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Car-Info </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这是第一篇本站的第一篇文章</title>
      <link href="/2023/11/04/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/"/>
      <url>/2023/11/04/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="创站帖"><a href="#创站帖" class="headerlink" title="创站帖"></a>创站帖</h1><p>本站建立于<strong>2023-11-4</strong><br>由<strong>Damon</strong>创建，作为私人博客</p><p>本站基于<strong>Hexo</strong>架构<br>使用<strong>butterfly</strong>主题<br>基于<strong>github</strong>、<strong>vercel</strong>实现网页代码托管<br>使用<strong>阿里云</strong>实现国内访问的映射</p><p>主站：wxtcon.github.io<br>vercel映射：wxtcon.vercel.app<br>阿里云映射：<a href="http://www.damonai.cn/">www.damonai.cn</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
